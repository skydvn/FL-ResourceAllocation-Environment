{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FL_NetworkEnvironment_Swe.ipynb","provenance":[],"collapsed_sections":["YbWesj-EqVJH","AlZWzW2aqkgn"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vrYpB8oFqMeF"},"source":["# **Initial Setting**"]},{"cell_type":"markdown","metadata":{"id":"8_7a2by5gO7M"},"source":["## 0. Import packages"]},{"cell_type":"code","metadata":{"id":"PL8vKnQmpZYH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636346286548,"user_tz":-540,"elapsed":8063,"user":{"displayName":"Dương Minh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17555676595979985239"}},"outputId":"6da1a103-0271-467f-c1bb-3354f9b2c515"},"source":["!pip install emnist\n","# dataset\n","from emnist import list_datasets\n","from emnist import extract_training_samples\n","\n","# complement\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import logging\n","import random\n","import time\n","import os\n","from scipy.special import softmax\n","\n","# tensorflow\n","import keras\n","import tensorflow as tf\n","from keras.utils import np_utils\n","from tensorflow.keras import layers, losses\n","from tensorflow.keras.datasets import fashion_mnist\n","from tensorflow.keras.models import Model\n","from keras.datasets import cifar10\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import Flatten\n","from keras import backend as K\n","from keras.constraints import maxnorm\n","from tensorflow.keras.optimizers import SGD\n","from keras.layers.convolutional import Conv2D\n","from keras.layers.convolutional import MaxPooling2D\n","\n","# sklearn\n","from sklearn.datasets import make_classification\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import auc\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import train_test_split"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: emnist in /usr/local/lib/python3.7/dist-packages (0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from emnist) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from emnist) (4.62.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from emnist) (1.19.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->emnist) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->emnist) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->emnist) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->emnist) (2021.5.30)\n"]}]},{"cell_type":"markdown","metadata":{"id":"sdsTm0F2w7Zr"},"source":["## 1. Setup dataset"]},{"cell_type":"code","metadata":{"id":"chm7Pst4w7Ab","executionInfo":{"status":"ok","timestamp":1636346690719,"user_tz":-540,"elapsed":530,"user":{"displayName":"Dương Minh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17555676595979985239"}}},"source":["class datasetSetup:\n","  def __init__(self, verbose=False):\n","    self.mode = \"MNIST\"\n","    self.verbose = verbose\n","    self.images, self.labels = self.downloadEmnist()\n","    self.check = [0] * self.labels.shape[0]\n","    # num of clients in each region\n","    self.num_of_clients = [1, 100, 100, 100]\n","\n","  def downloadMNIST(self):\n","    (train_X, train_y), (test_X, test_y) = tf.keras.datasets.mnist.load_data()\n","    if self.verbose == True:\n","      logging.infor(train_X.shape, train_y.shape)\n","    return train_X, train_y, test_X, test_y\n","\n","  def downloadEmnist(self):\n","    images, labels = extract_training_samples('balanced')\n","    if self.verbose == True:\n","      logging.info(images.shape, labels.shape)\n","    return images, labels      \n","\n","  def preProcessData(self, N_user):\n","    # Train-Test\n","    if (self.mode == \"MNIST\"):\n","      self.X_train, self.y_train, self.X_test, self.y_test = self.downloadMNIST()  \n","      print(len(self.X_train))\n","      print(len(self.X_test))\n","    elif (self.mode == EMNIST):\n","      x, y = self.downloadEmnist()\n","      self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(x, y, test_size=0.05, random_state=42)\n","      \n","    # Data Normalization\n","    raw_dataset_for_iid  = list(zip(self.X_train.reshape(-1, 28, 28, 1).astype(\"float32\")/255.0, self.y_train.astype(\"float32\")))\n","    test_dataset_for_iid = [self.X_test.reshape(-1, 28, 28, 1).astype(\"float32\")/255.0, self.y_test.astype(\"float32\")]\n","    random.shuffle(raw_dataset_for_iid)\n","\n","    # Check X_train total data + split by number of user:\n","    total_samples = len(self.X_train)\n","    num_of_samples = total_samples/N_user\n","   \n","    # Assign data to clients\n","    temp_list_for_image=[]\n","    temp_list_for_label=[]\n","    federated_train_data_for_iid = []\n","    federated_test_data_for_iid = []\n","    for idx, el in enumerate(raw_dataset_for_iid) :\n","        temp_list_for_image.append(el[0])\n","        temp_list_for_label.append(el[1])\n","        if (idx+1)%(num_of_samples)==0 :\n","            federated_train_data_for_iid.append((np.array(temp_list_for_image, dtype=\"float32\"), np.array(temp_list_for_label, dtype=\"float32\")))\n","            temp_list_for_image=[]\n","            temp_list_for_label=[]    \n","\n","    federated_test_data_for_iid = test_dataset_for_iid\n","\n","    return federated_train_data_for_iid, federated_test_data_for_iid\n","\n","  def visualizeData(self, image):\n","    for i in range(9):\n","      # define subplot\n","      plt.subplot(330 + 1 + i)\n","      # plot raw pixel data\n","      plt.imshow(image[i].reshape(28,28))\n","      # show the figure\n","    plt.show()    "],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XtmpL0TTqQHe"},"source":["# **Federated Learning Definition**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5E_gV1phsVqq"},"source":["## BaseNetwork"]},{"cell_type":"code","metadata":{"id":"1Hq4XlA_sYAu","executionInfo":{"status":"ok","timestamp":1636346419539,"user_tz":-540,"elapsed":469,"user":{"displayName":"Dương Minh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17555676595979985239"}}},"source":["class BaseNetwork(Model):\n","  def __init__(self):\n","    super(BaseNetwork,self).__init__()\n","    self.mode = \"MNIST\"\n","    if (self.mode == \"MNIST\"):\n","      self.predictor= tf.keras.models.Sequential([\n","        tf.keras.Input(shape=(28, 28, 1)),\n","        tf.keras.layers.Conv2D(128, kernel_size=(5, 5), activation=\"relu\", padding='same'),\n","        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'),        \n","        tf.keras.layers.Conv2D(64, kernel_size=(5, 5), activation=\"relu\", padding='same'),\n","        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),        \n","\n","        tf.keras.layers.Flatten(),\n","        tf.keras.layers.Dense(512, activation='relu'),\n","        tf.keras.layers.Dense(10, name = \"logits\"),\n","        tf.keras.layers.Activation('softmax')\n","        ])\n","    elif (self.mode == \"EMNIST\"):\n","      self.predictor= tf.keras.models.Sequential([\n","        tf.keras.Input(shape=(28, 28, 1)),\n","        tf.keras.layers.Conv2D(128, kernel_size=(5, 5), activation=\"relu\", padding='same'),\n","        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'),        \n","        tf.keras.layers.Conv2D(64, kernel_size=(5, 5), activation=\"relu\", padding='same'),\n","        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),        \n","\n","        tf.keras.layers.Flatten(),\n","        tf.keras.layers.Dense(512, activation='relu'),\n","        tf.keras.layers.Dense(47, name = \"logits\"),\n","        tf.keras.layers.Activation('softmax')\n","        ])\n","\n","  def call(self, x):\n","    predictor = self.predictor(x)\n","    return predictor"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BbPZkmWRqoqv"},"source":["# **Channel Environment**"]},{"cell_type":"code","metadata":{"id":"r__6EYnMsynH","executionInfo":{"status":"ok","timestamp":1636347946469,"user_tz":-540,"elapsed":532,"user":{"displayName":"Dương Minh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17555676595979985239"}}},"source":["class FL_Environment:\n","  def __init__(self,\n","               areaR, x_0, y_0, N_user,                 # Area settings\n","               P_user, B_user,                          # User settings\n","               P_server, B_server,                      # Server settings\n","               fraction, total_user,                    # FL settings\n","               samples_region_train,\n","               samples_region_test,\n","               batch_size = 32, epochs = 50,\n","               verbose=True\n","               ):\n","\n","    # FL dataset\n","    self.samples_region_train = samples_region_train\n","    self.samples_region_test  = samples_region_test\n","    # Aggregated model initialization\n","    self.server_aModel = BaseNetwork()\n","    self.server_aModel.predictor.compile(optimizer = 'adam',\n","                                         loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                                         metrics = ['accuracy'])\n","\n","    # FL client settings: \n","    self.regional_dModel = BaseNetwork()\n","    self.regional_dModel.predictor.compile(optimizer = 'adam',\n","                                           loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                                           metrics = ['accuracy'])\n","    self.total_clients = len(samples_region_train)\n","    self.selected_clients = int(self.total_clients*fraction)                       \n","\n","    # Area settings\n","    self.R = areaR\n","    self.N = N_user\n","    self.B_user = B_user\n","    # Server settings\n","    self.x_0 = x_0\n","    self.y_0 = y_0\n","    # self.userList_Radius = []\n","    # self.userList_Angle = []\n","    self.userList_x = []\n","    self.userList_y = []\n","    self.userList_location = []\n","    \n","    # General settings \n","    self.shuffle_per_round = True \n","    self.vers = verbose\n","    if self.vers == True:\n","      self.regional_dModel.predictor.summary()\n","    self.batch_size = batch_size\n","    self.epochs = epochs                       \n","\n","  def _clientInit(self, ):\n","    for user_no in range(self.N):\n","      user_Radius = random.uniform(50, self.R)\n","      user_Angle  = 360*user_no/int(self.N)+random.uniform(-5, 5)\n","      user_x = self.x_0 + user_Radius * math.cos(math.radians(user_Angle))\n","      user_y = self.y_0 + user_Radius * math.sin(math.radians(user_Angle))\n","\n","      self.userList_x.append(user_x)\n","      self.userList_y.append(user_y)\n","      self.userList_location.append((user_x, user_y))\n","    # Client Plot\n","    fig, ax = plt.subplots()                                                   \n","    circle1 = plt.Circle((self.x_0, self.y_0), self.R, color='b', fill=False) \n","    circle2 = plt.Circle((self.x_0, self.y_0), 1, color='r', fill=True)       \n","    ax.add_patch(circle1)\n","    ax.add_patch(circle2)\n","    print(f\"List of user location is: {self.userList_location}\")\n","    plt.scatter(self.userList_x, self.userList_y)\n","    plt.show()\n","\n","  \"\"\"\n","     Federated Learning Process   \n","  \"\"\"\n","  def _clientTrain(self, global_params):\n","    list_of_local_parameter=[]       # Distributed model   \n","    list_of_local_dataset_size=[]    # Distributed model   \n","    list_of_local_accuracy=[]        # Distributed model   \n","    list_of_local_loss=[]            # Distributed model  \n","    if self.vers == True: \n","      print(\"\\n▶ Round\", round+1, \"◀\")\n","    # check whether to apply shuffle mode per round\n","    if self.shuffle_per_round == True:\n","      selected_clients_list = np.random.choice(self.total_clients, size=self.selected_clients, replace=False) \n","\n","    for client_idx in selected_clients_list:\n","      # get data for each cliente\n","      train_images, train_labels = self.samples_region_train[client_idx]\n","      # update teacher params\n","      self.regional_dModel.predictor.set_weights(global_params)\n","      print(\"=====================\")\n","      print(f\"train label is: {train_labels.shape}\")\n","      train_result = self.regional_dModel.predictor.fit(train_images,\n","                                                       train_labels,\n","                                                       self.batch_size,\n","                                                       self.epochs)\n","      print(\"=====================\")\n","      list_of_local_parameter.append(self.regional_model.predictor.get_weights())\n","      list_of_local_dataset_size.append(len(train_images))\n","      list_of_local_accuracy.append(train_result.history[\"accuracy\"][-1])\n","      list_of_local_loss.append(train_result.history[\"loss\"][-1])\n","\n","    return list_of_local_parameter, list_of_local_dataset_size, list_of_local_accuracy, list_of_local_loss\n","\n","  def _ServerAggregate(self, list_of_local_parameter, list_of_local_dataset_size, list_of_local_accuracy, list_of_local_loss):\n","    self.global_parameter = np.mean(list_of_local_parameter, axis=0)\n","    current_mean_accuracy = np.mean(np.array(list_of_local_accuracy, dtype=\"float32\"))\n","    current_mean_loss = np.mean(np.array(list_of_local_loss, dtype=\"float32\"))\n","    if self.vers == True:\n","      print(f\"  evaluation mean : accuracy - {current_mean_accuracy}, loss - {current_mean_loss}\")\n","      self.server_aModel.predictor.set_weights(global_params)\n","      self.server_aModel.predictor.evaluate(x = self.samples_region_test[0], y = self.samples_region_test[1])\n","    return self.global_parameter\n","\n","  ####################################################################################################\n","  \"\"\"\n","     Channel Propagation Process   \n","  \"\"\"\n","  def client2server_env(self, dataModel):\n","    self.OTAtrigger = False\n","    \"\"\"\n","      If OTA FL, accuracy reduce, we use it here later\n","    \"\"\"\n","    return dataModel\n","  \n","  ### =============================== ###\n","  def server2client_env(self, dataModel):\n","    self.OTAtrigger = False\n","    \"\"\"\n","      If OTA FL, accuracy reduce, we use it here later\n","    \"\"\"\n","    if heterModel: \n","      pass\n","    else: \n","      return dataModel\n","\n","    return dataModel\n","\n","\n","  ####################################################################################################\n","        ######################################################################################\n","  def _propagationLoss(self):\n","    \"\"\"\n","    Parameters:\n","    ===========\n","      userList_distance: List of user's distance, calculated by norm-2 between user and server \n","    ===========\n","    \"\"\"\n","    userList_distance = []            \n","    # init BS location:               \n","    bs_location = (self.x_0, self.y_0)\n","\n","    for user_no in range(self.N):\n","      user_distance     = np.linalg.norm(np.array(self.userList_location[user_no]) - np.array(bs_location))\n","      userList_distance.append(user_distance)\n","    print(f\"List of user distance is: {userList_distance}\")\n","    self.userList_pathloss = 140.7 + 36.7 * np.log10(userList_distance)\n","\n","    return self.userList_pathloss, userList_distance\n","\n","  def _channelGain(self):\n","    self.heterModel = False\n","    \"\"\"\n","      If heterogeneous model, accuracy reduce, we use it here later\n","    \"\"\"\n","    if self.heterModel: \n","      self.userList_H = self.userList_pathloss  \n","    else: \n","      self.userList_H = self.userList_pathloss   \n","\n","    return self.userList_H\n","\n","        ######################################################################################\n","  def _dataRate(self, bandwidth, channel, powerBS, powerUser, noise):\n","    self.total_bandwidth = bandwidth\n","    self.resource_coeff  = np.array(channel)\n","    self.powerBS         = powerBS\n","    self.powerUser       = np.array(powerUser)\n","    self.noise           = noise\n","\n","    userList_datarate = abs(self.total_bandwidth * self.resource_coeff * \n","                            np.log2(1 + ((self.powerUser * self.userList_H) / \n","                                          (self.noise * self.resource_coeff + self.powerBS * self.userList_H))))\n","\n","    return userList_datarate"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e44MgThBrNSy"},"source":["# **Federated Learning Process**"]},{"cell_type":"markdown","metadata":{"id":"eK1DMdT6uwuA"},"source":["## FL Initialization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"nt0EaP4Ynrta","executionInfo":{"status":"ok","timestamp":1636347951499,"user_tz":-540,"elapsed":2238,"user":{"displayName":"Dương Minh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17555676595979985239"}},"outputId":"679eff34-afcc-42be-86e1-e0953ee9d85f"},"source":["fraction = 0.1\n","total_user = 100\n","\n","################################## Initialization ####################################\n","bandwidth = 20*(10**6)\n","resource_coeff = []\n","for user_no in range(total_user):\n","  resource_coeff.append(1/(fraction*total_user))\n","powerBS = 100\n","powerUser = 5\n","noise = 5\n","TRAINING_ROUNDS = 5\n","initModel = BaseNetwork()\n","get_serverModel = initModel.predictor.get_weights()\n","\n","######################################################################################\n","dataset = datasetSetup()\n","federated_train_data_for_iid, federated_test_data_for_iid = dataset.preProcessData(total_user)\n","######################################################################################\n","FL_Env = FL_Environment(areaR = 1000, N_user = 100, P_user = 1, B_user = 2, P_server = 3, B_server = 4, x_0 = 0, y_0 = 0,\n","                        fraction = fraction, total_user = total_user,\n","                        samples_region_train = federated_train_data_for_iid, samples_region_test = federated_test_data_for_iid)\n","FL_Env._clientInit()\n","setPathloss, setDistance = FL_Env._propagationLoss()\n","setChannelGain = FL_Env._channelGain()\n","setDatarate = FL_Env._dataRate(bandwidth= bandwidth, channel= resource_coeff, \n","                               powerBS= powerBS, powerUser= noise, noise= noise)\n","print(f\"List of user distance is: {setDistance}\")         \n","print(f\"List of user path loss is: {setPathloss}\")        \n","print(f\"List of user channel gain is: {setChannelGain}\")  \n","print(f\"List of user data rate is: {setDatarate}\")        \n","\n"],"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["60000\n","10000\n","Model: \"sequential_23\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_46 (Conv2D)           (None, 28, 28, 128)       3328      \n","_________________________________________________________________\n","max_pooling2d_46 (MaxPooling (None, 14, 14, 128)       0         \n","_________________________________________________________________\n","conv2d_47 (Conv2D)           (None, 14, 14, 64)        204864    \n","_________________________________________________________________\n","max_pooling2d_47 (MaxPooling (None, 7, 7, 64)          0         \n","_________________________________________________________________\n","flatten_23 (Flatten)         (None, 3136)              0         \n","_________________________________________________________________\n","dense_23 (Dense)             (None, 512)               1606144   \n","_________________________________________________________________\n","logits (Dense)               (None, 10)                5130      \n","_________________________________________________________________\n","activation_23 (Activation)   (None, 10)                0         \n","=================================================================\n","Total params: 1,819,466\n","Trainable params: 1,819,466\n","Non-trainable params: 0\n","_________________________________________________________________\n","List of user location is: [(765.1483900131743, -50.32284159206165), (385.2886974654591, 46.10009922664609), (370.22513986498836, 16.441853733389188), (250.01134874199002, 56.005412631244866), (88.49505541021401, 22.785977371621698), (551.376742394897, 180.90514077304996), (252.74067386750534, 97.18481273252416), (97.56496594306668, 41.64089128199667), (516.245802217605, 267.9099172962382), (260.2774083285041, 187.39437296258393), (83.3060543962309, 66.80879349611584), (642.0439514859138, 460.923766152084), (129.5456347645239, 110.51360780542062), (457.824235454307, 466.2250955087303), (278.99693337027577, 402.14431177960967), (52.28028872541533, 71.1712761529249), (235.91105852478137, 318.3474728671442), (481.686650241858, 816.7463387929718), (164.11445587341746, 306.22823722064874), (104.39799036844724, 236.35489055751057), (59.88047641944773, 244.80330362187433), (100.79659645907964, 353.29923776141146), (60.52581235034028, 416.1864130529179), (109.04715861993353, 515.6289170283892), (3.6778877753675046, 354.7175289878751), (-14.01096644422888, 675.0906035084059), (-130.19483607048008, 864.040546680826), (-105.18034448842286, 852.6803968469051), (-26.636073547778096, 122.3228376926053), (-84.87950000846098, 461.95555781665917), (-128.3188649304374, 394.43701902527454), (-166.6832633550036, 334.97478159052963), (-263.58865344541107, 457.6962782750578), (-290.9913446261558, 619.7921797451693), (-378.512745514098, 566.7071656538319), (-506.22113133725463, 634.8506031054442), (-477.74942722461094, 509.3788527548504), (-588.7578581554843, 556.0577407269884), (-233.53102260891185, 226.40441447695383), (-492.68709613272136, 413.90387672381087), (-143.2417930224088, 92.22632655951773), (-263.749885266159, 161.1052707905986), (-735.989378620314, 396.1144418521832), (-694.7483484578346, 341.58553612932286), (-417.7329595490163, 143.14019454070527), (-501.54733209336865, 156.6631504258221), (-841.038466235431, 175.39704710801306), (-404.99816183306365, 101.88492570068932), (-547.8833202022828, 83.3178340677134), (-126.22734523739165, 6.39845887339951), (-148.3652616551235, 3.3504592103468473), (-157.70617582916685, -21.02628498326734), (-579.9627464406238, -89.53028474934803), (-598.8552916218631, -134.84401719602218), (-362.6256872773965, -108.04888426932565), (-198.85182110622824, -71.52551950179044), (-118.4753545069731, -49.2669107488645), (-539.487839085765, -313.85615690146034), (-48.846945148147135, -24.035370272916715), (-457.13024569130897, -295.69435556677996), (-758.5916212489597, -474.49696377357805), (-693.037277214579, -495.7878823336705), (-364.8422672628325, -388.3882353947588), (-345.668886439578, -414.1440436317697), (-383.96681619430694, -394.4070259754071), (-497.51975851021314, -578.8108119377544), (-235.86674910607277, -316.924456188635), (-195.58912162629423, -417.2677682588243), (-364.81795807610104, -887.8761232994041), (-276.24457270552864, -698.0152155156975), (-72.78648278878589, -267.3527244251057), (-267.2298273103774, -831.2776153969638), (-19.953876797282064, -173.1603303647965), (-40.55165275883653, -506.2520921562994), (-38.88912655501835, -439.69634275730186), (2.99592539518715, -865.7571810862149), (71.9929219209147, -725.5712171554266), (25.45833510186038, -179.463126044219), (76.7853377015033, -288.5948829746027), (38.16869465896066, -155.09559595216317), (18.887128085641816, -54.285678054785535), (403.0994693401894, -857.9115063901322), (27.590701967657424, -60.54227709470404), (350.9195554321742, -551.4315026253713), (177.55587537050167, -260.5215063433393), (239.68365981674424, -313.301367220083), (222.52247952908962, -280.6762880189892), (154.2301086044816, -167.25098879137346), (568.9200001611066, -474.95134413965957), (504.5394212241482, -417.6783172065707), (201.48847582861433, -123.20213229293027), (311.8519763054237, -220.9737644637197), (205.9761895483865, -134.31138781210973), (489.9472892244604, -270.646265144333), (875.5434019546135, -305.52756939556957), (464.9205390012621, -162.41830420796782), (907.0745291803959, -163.14977412427982), (109.0081205109829, -11.412270874241498), (958.7549532757629, -170.30002974382853), (335.892646531041, -36.45013187391163)]\n"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deZgU1dW438MqggooEhhAQAEVF5Bx3xBRwQ1cohg/l5i4689oRHFJokmM+BFjjPFDcdcYFhWBKIYAbomKsqNssisDIooIAhmWub8/TjXTQHdP90xVV1X3eZ+nn+m+vdSZW9331D2rOOcwDMMwDIBaYQtgGIZhRAdTCoZhGMZ2TCkYhmEY2zGlYBiGYWzHlIJhGIaxnTphC1BT9tlnH9e2bduwxTAMw4gVU6dO/cY512zn8dgrhbZt2zJlypSwxTAMw4gVIrIs1biZjwzDMIztmFIwDMMwtmNKwTAMw9iOKQXDMAxjO6YUDMMwjO34ohRE5FkR+VpEPksaayoi40Vkgfe3iTcuIvIXEVkoIrNE5Iik91zhvX6BiFzhh2yGYRhG9vi1U3ge6LXT2ABgonOuAzDRewzQG+jg3a4BBoMqEeA3wNHAUcBvEorEMAzDyA++5Ck4594XkbY7DfcBunv3XwDeBe70xl90WrN7kog0FpEW3mvHO+fWAIjIeFTRDPVDRsPwA+fghx9g7Vr4/nvYsgUqKmDbtspb4rFzULt25a1Wrcr7derAHntA48aw5576nGFEgSCT15o751Z6978Cmnv3S4Avk1633BtLN74LInINusugTZs2PopsFAvOwZo1sGwZfPMNfPedLvRV/V27FurVgyZNdDGvV2/XBT/xWGRXRZG4bd0K69fr527YoJ/VpIkqieS/6cbatIEf/ciUieE/eclods45EfGtm49zbggwBKC0tNS6BBkp2bgRli6FxYthyZLKW+JxrVrQti00a7br4tu27a6LcePGeqtXz185t27VXUcmZbR8eeXjb7+FL75QpbLfftCund7at6+8366dymoYuRKkUlglIi2ccys989DX3ngZ0Drpda28sTIqzU2J8XcDlM8oADZsgBkzYN68XRf977/fddE8/vjKx00i4rGqUwf23ltvubBhw47KbskS+Pe/K///unV3VBLt2kHHjtC1a+7HMoqHIJXCGOAKYKD3d3TS+E0iMgx1Kn/vKY5xwB+SnMunA3cFKJ8RM9atUwUwdSpMm6Z/ly6Fzp311r499OpVuQC2aFHY5pWGDeGQQ/S2M87pjiJZSc6cCcOHw/Tp0LQpdOsGRxxR+XffffP/PxjRQ/zo0SwiQ9Gr/H2AVWgU0ShgBNAGWAZc5JxbIyIC/BV1Im8Efuqcm+J9zlXA3d7HPuCce66qY5eWljoriFd4rF2ri9fUqZVKYPlyOOywHReyzp31itjInooKWLiwUrFOm6a3Ro12VRQtWoQtrREUIjLVOVe6y7gfSiFMTCnEn4oKmDULJk6Ejz/WBWrVKjj88B0XqQMPVFOL4T/O6W4ieRc2dSrUr6/zf+SR0KMHHHOMKeFCwZSCESmWL4fx4/U2cSLstRf07AnHHacKoGNHjeIxwsM5dWhPm6bKesIEWLAATjpJz9Vpp8FBB2mUlRE/TCkYobJ+Pbz7bqUiWL0aTj1VF5bTTlOHsBF9vvkG3n678jxu3VqpIHr2hObNq/4MIxqYUjDyytat8MknlYvHzJlw9NGVi0fXroXtBC4GnFPfROIcv/uu5k8kFP2JJ8Luu4ctpZEOUwpG4JSXw1tvwbBh8M9/aqx/YoE44QRbIAqdrVth8uRKJTF9uoYA9+sH551neRNRw5SCEQhbt6o5YdgwGDVKo4MuuQT69jVTQrGzfr1eJAwdqt+RU07R78Y559gFQhQwpWD4RkUFfPSR/thfeUX9AZdcAhddBCUpC5NEm1HTyxg0bj4r1m6iZeMG9D+jE327xvAfiTDffw+vv67fmY8/hrPO0h3EGWf4nyFuZIcpBaNGOKeJY0OHagJUo0aqCPr1gwMOCFu66jNqehl3jfyUTVu2bR+rW0totFsd1m7cYkoiAL7+Gl59Vb9Lc+bA+efrd+nkky3iLJ+YUjCqxeLF8NJL+gPevFmVwCWXwKGHhi2ZPxw/8G3K1m7K+JoGdWvz4PmHmmIIgC++gBEj9Pu1YoXuNi+7DEp3WaoMv0mnFCz+w9gF5zR34NxzNWLou+/ghRdg0SL4wx8KRyEArKhCIQBs2rKNQePm50Ga4qNNG7j9dk2Ue+89rcl00UWaJDd0qJYmN/KLKQVjOxs3wpAhuujfcos6BJctgz//WZVDISYptWzcIKvXZaM8jOwZNb2M4we+TbsBb3L8wLcZNb2Mjh3h17/WBLm77oKnntIaVg88oHktRn4wpWDw5ZcwYIA6jN94Ax59FD79FK6+uvCjRPqf0YkGdas2ZGerPIyqSfhxytZuwgFlazdx18hPGTW9DFC/Qp8+GrE0dqyW3+jYEa66SvNdjGAxpVCkOAf/+Y9u1Q8/HP77X40oGjNGM40LcVeQir5dS3jw/EMpadwAARo3qEvd2jv+8w3q1qb/GZ3CEbAAGTRu/g6OfUhvojvsMHj6ad09HHCARi11766RTNu27fJywwfM0VxklJdr9NCjj2op6ptvhiuv1M5fhmIhqsHSbsCbpFp1BFgy8KyM792yBUaO1O/vihVw003ws59FpzdGnEjnaLaak0XC5s1qo33gAa2//9vfQu/eVmoiFX27lpgSCJCWjRukjPjKxkRXty5cfLHeJk9W5bD//nDddXDHHZY17Qe2JBQ4FRXw8stadvqNN+DNN+Ff/9JtuCkEIwxS+XGqY6I78kj429/Uz/D11+p3GDQINllMQI2wZaFAcU6ddF27wmOPwbPPasmBrl3Dlswodnb245Q0blCjPJDWrdXv8N57MGkSdOigu+KtW/2Vu1gI1KcgIp2A4UlD7YFfA42Bq4FEoNndzrmx3nvuAn4GbAP+n3NuXKZjmE9hVz78UKOJvvlG8wr69Ckex7FhfPKJfv/LytRcesEF9v1PRegZzSJSGyhD+zL/FPjBOffHnV5zMDAUOApoCUwAOjrn0sYZmFKo5LPP4J57tBzF/fdrZqiVDTCKEee0KdCAAWomffBBLdluVBKFjOZTgUXOuWUZXtMHGOacK3fOLQEWogrCyMDSpXDFFRpK2r07zJ+vEUWmEIxiRURLtk+eDP37w/XX62O7fqyafCqFfuguIMFNIjJLRJ4VkURAWQnwZdJrlntjOyAi14jIFBGZsrqIUx1/+EG/8N26ae+CBQvg1ltht93ClswwokGtWpqLM2cOXHihmlIvvlhNS0Zq8qIURKQecC7wijc0GNgf6AKsBB7O5fOcc0Occ6XOudJmzZr5KmtcePNNDS396iv9wt9/v+UaGEY66taFa6/VC6eOHaFLF3j8cUuAS0W+dgq9gWnOuVUAzrlVzrltzrkK4CkqTURlQOuk97XyxgyPlSv1yueWWzTC4qWXrJmNYWTL7rvD736nrUOHDtXOcLNmhS1VtMiXUriEJNORiLRIeu484DPv/hign4jUF5F2QAfgkzzJGGkqKuCJJzTtv0MHrU102mlhS2UY8aRzZ3j/fc2G7tlTHdKW36AEntEsIg2B04Brk4b/V0S6AA5YmnjOOTdbREYAc4CtwI2ZIo+KhaVLtRjYxo3wzjtqNjKih5XHiBe1amnRx3PO0Z13167w3HNw7LFhSxYuVvsowjinpazvvVdT+G+7zSKKokqqDm7WnCdevPqq1lK6/HItA1PoARtRCEk1cuCLL+D00+GZZzRTs39/UwhRJpfKn0Y0ufBC9S8sXgxHHKFJcMWIKYUIMny4hpmeeqpmJx98cNgSGVWRrgmPNeeJF/vuC6+8Ar/5jZqVfvtb9ecVE1YlNUJs3aoOr5EjtWhdFOoUmZ08O2pS+dOIFiKay3DSSfDjH2vC20svwV57hS1ZfrCdQkRYvVrNRZ9+qlmYUVEImTpkGZX4VfnTiA4tWmj3tzZttCLr7NlhS5QfTClEgClToLRU+yCPHavNy6NAVOzkqfr5Rg2/K38a0aBePfjrX7WmWPfu6owudMx8FDLPPaeRRU88odUco0QU7OQ7R/UkditA5BZca85TuFxxhYaCX3CBXsQ98EDhBn7YTiEkNm+GG26AgQM1uihqCgHS28PzaSePym7FMLp1U9Pu5MnQqxd8+23YEgWDKYUQWLECTjlFi3J98kl0o4uiYCePwm7FMBI0awbjxmntpNJSmD49bIn8x5RCnvnwQ3Va9eoFr78e7YiGKNjJo7BbMYxk6tTRtp8DB2pwyN/+FrZE/mI+hTwybpw2vnnuOe2RHAf8tJNXJ7y1/xmdUmYKW1SPETYXX6w1lM45R6MHb701bIn8wZRCnvjHP7T41qhRcNxx4ckRVt5BdR3GiecsV8KIIoccoj7BU0+F8nLNM4o7phTywMiR2vnpjTfgqBD7yIUZyZPJYVzVsS2qx4gybdpoKe5TT9UAkl/9Kt49oc2nEDDDh2uU0VtvhasQINxIHnMYG4VMSYkqhuHDVSnEuc6oKYUAeekltTOOH68FtsImzIXZHMZGofOjH6lieOMNzT2Kq2IwpRAQzz4Ld90FEybAoYeGLY0S5sIchfDWOBGHLG5jV5o109IY77wDv/hFPBWDKYUAGDwY7rtPvxxRykHwY2Gu7mIVhfDWuGA1p+JN06Z6Mfjxx+pLjFuV1cCb7IjIUmA9sA3Y6pwrFZGmwHCgLdp57SLn3HciIsCjwJnARuBK59y0TJ8ftSY7jz4KjzwCEyfC/vuHLc2u1CT6yBrJ5IfjB76dsuJqSeMGfDCgRwgSGdVh3ToNPe/QQfupR60sRromO/mKPjrFOfdN0uMBwETn3EARGeA9vhPojfZl7gAcDQz2/saCv/9dFcJ778F++4UtTWpqEslTkwgiI3vMKV8Y7LmnBpicdZaGqg4aFLZE2RGW+agP8IJ3/wWgb9L4i06ZBDQWkRZhCJgrkydrn9cxY6KrEGqKLVb5wZzyhUOjRhqSPnIkvPhi2NJkRz6UggP+JSJTReQab6y5c26ld/8roLl3vwT4Mum9y72xHRCRa0RkiohMWb16dVByZ83KlXD++dpP+bDDwpYmOIpxsQrD4VuoTvlidZ7vvbdeLN5+O0yaFLY0VZMP89EJzrkyEdkXGC8i85KfdM45EcnJseGcGwIMAfUp+Cdq7vz3v9C3L1x7LZx3XpiSBE9USk7kKys7rGS/QsziDmMuo9Q1sHNn7bd+wQXqgG7VKhQxsiJwpeCcK/P+fi0irwNHAatEpIVzbqVnHvrae3kZ0Drp7a28sUjiHFxzDbRtq004Cp0oLFb5XFzC9KEUWhZ3vucyin04zjlHu7f17Qvvvw+77x6KGFUSqPlIRBqKyB6J+8DpwGfAGOAK72VXAKO9+2OAy0U5Bvg+ycwUOR5+WE/yc8/FO609F/p2LeGDAT1YMvAsPhjQI+8/sHxmZZsPxT/yPZdR7cNx551w4IFaBy2qOQxB+xSaA/8RkZnAJ8Cbzrl/AgOB00RkAdDTewwwFlgMLASeAm4IWL5qM3asRhqNGhVdjV+I5HNxKUYfSlDkey6jqtBFNDx10SJ48MFQRUlLoOYj59xi4PAU498Cp6YYd8CNQcrkB3PnwpVXwujR0Lp1lS8vCKJin23ZuEHKGP4gFpcgfShRmc98kW9/VD6/J7nSoIH2Ujn6aPU19OkTtkQ7YhnNObJ+vZ7EQYPg2GPDliY/RCnDNp+ROUFlYUdpPvMVEZTvjPaoR3CVlGiY6s9/DvMj1lk28IzmoMl3RvMtt8D338Pzzwd3jKhdRUYtwzaI+cnnnEdlPgs9Qz1qv6NUPPYYjBihCa+18nyJHnZGc0Hw8cd6Aj/7LLhjRDFqImr2Wb8jc/I951GZz0LPUI9DBNcNN2glhKee0rD2KGDmoyzZvFm3eo88oskoQRHFqIlCd7jme86jMp9RUU7FTO3aqhDuvRdWrAhbGsWUQpYMGqQdli6+ONjjRPGHGnX7bE3J95xHZT6jopyKnUMOgeuug5tvDlsSxZRCFnz+ue4QBg8OPh8hij/UQi97HcScZ3LgZprPfJaCiIpyMjT5dfZsjUoKG3M0V0FFBfTooSUsbrklsMNsJ5PzDwqr9EFU8NvhWt3PC8PxGwdnbLHw/vvwk5+octhrr+CPl87RbEqhCp56Cp5+Gj78MH/10FP9UIGCjhTJF+kWQT8Xx+pGF0UlKskIj2uv1SikwYODP5ZFH1WDlSt1WzdxYn4bZKSKmjh+4NsFHSmSD6qKMvJrHqvro4iiP8nILw89pAltl14KJ5wQjgzmU8jAL38JV18djR7LtmDUnHxFGVXXRxGUP6lYS1bHkcaN4S9/0UKb27ZV/fogMKWQhs8/h/Hj4a67wpZEiaIDOm7kS7FW14Gby/uyXeijlD1tZMf556tyGDkynOObUkjDoEFw443aOSkKWKRIzcmXYq1utFa278tloY9i3ouRGRFt3zlwYDiVVM3RnIIVKzR2+PPPYZ99fP3oGmGRIjWjUMo65OKQbjfgTVL9wgVYMvCsYAQ0akxFhZqtH30UevYM5hjmaM6BP/8ZLrssWgoB4pG2H2Wi0CTID3Ixg0W5WqiRnlq14I47dLcQlFJIhymFnfjuO22bN21a2JIYQVAIijWXhT4qLVSzxXbDlVxyCfzqVzB5Mhx5ZP6OG5hPQURai8g7IjJHRGaLyC3e+H0iUiYiM7zbmUnvuUtEForIfBE5IyjZMjF4MJx9Nuy3XxhHN4yqycW/FKdsdHOK70i9ehoB+dBD+T1uYD4Fr/dyC+fcNK8l51SgL3AR8INz7o87vf5gYCjaw7klMAHo6JzLGJjlp09h0yZo107zEjp39uUjDSMlNb0iLsQrakve25UNG3RN+ve/oZPPm7u8+xS83sorvfvrRWQukOlb2wcY5pwrB5aIyEJUQXwUlIw789xzld2QDCMoqkqiy2bBLwQz2M5YLs6uNGyoUZCDBmllhXyQl5BUEWkLdAU+9oZuEpFZIvKsiDTxxkqAL5Petpw0SkRErhGRKSIyZfXq1b7IuG2bTvydd/rycUaeiVOCVqYw0WI2oVguTmpuuklzFvJVWjtwpSAijYDXgF8459YBg4H9gS7oTuLhXD/TOTfEOVfqnCtt1qyZL3J++CHsuSccd5wvH2fkkbgtpJmuiIs5r8BycVKz997Qty+88kp+jheoUhCRuqhCeNk5NxLAObfKObfNOVcBPIWaiADKgNZJb2/ljeWFMWN04o34EbeFNNMVcTGbUOLkFM83ffroGpUPgow+EuAZYK5z7k9J4y2SXnYekGhuOQboJyL1RaQd0AH4JCj5knEORo+Gc8/Nx9EMv4nbQprpirjYTSh9u5bwwYAeLBl4Fh8M6GEKwaNnTw1N/e674I8VZJ7C8cBlwKciMsMbuxu4RES6AA5YClwL4JybLSIjgDnAVuDGqiKP/GLePI08OuKIfBzN8Ju4JWhVlUQXdF5B3CKX4iZvEDRsCN27w9ixWkE1SKzMBZo1+OWX8Pjj/shkX+L8UijlKxIE+f2J21zFTd4geeYZGDcORozw5/OsyU4Gjj0W7r8fTj+95vLYlzh40jUhMkVcNXHLBYibvEGyapXmKqxaBfXr1/zzrPZRGr76CubO1a2ZH2RyesZ1kYrSziddjP+D5x9adItEdYib/yVu8gZJ8+aaQ/XOO9CrV3DHKfrS2f/4B5xxhqaU+0GhfYmrG+4ZVN5A3CKNokbcHNlxkzdozj1Xg2KCpOiVwujRGu7lF4X2Ja7OIhxk3kChKd18E7dcgLjJGzSJ0NSKiuCOUdRKYetW3Yr17u3fZxbal7g6i3CQV/NRVLpxyqaOWy5A3OQNmgMPhAYNYPbs4I5R1D6FpUuhWTNo0qTKl2ZNodTsT1CdcM8gr+ajVgq6qjpGfh3Dz+9T3OomxU3eoDnsMPWDBtU7vqiVwvz5/lcehML6EldnEU6nSPZqULfG8kRN6QYdWOC30olS0IBRPTp10rUrKEwpxNOqkzeqswj3P6MT/V+ZyZaKHcOdN2zeyqjpZTVehPxSun4skEH7OPxUOvnY1RjB06kTTJgQ3OcXvVI47LCwpYgGmRbIXBfhvl1LuP8fs/lu45Ydxrdsc5EJzfVrgQw6m9pPpVOI4dLFSKdO8Ne/Bvf5Ra8UfvzjsKUInyCuINfupBASRCVKyK8FsjrmtVyS7/xUOha5VRgkzEfOgYj/n1/U0UdmPlKCiBaKYpRQMn4tkLlGx6QK1+3/6kz6vzIzZQivn9FsUT8nRnY0bQq77QYrVwbz+UW7U/j+e1i/Hkps1xzIFWTUooR2xs8r8FzMa6kU8JZtu5aaSSjlRJa2H87hUw5sxsuTviD5aFE6J0b2HHigXtS2bOn/ZxetUpg/Hzp2hFpFvVdSgrCLRy1KaGfCUlq5KNqytZu2O+ZrOm+jppfx2tSyHRSCABd0q9lnWzRTOCRMSKec4v9nF7VSMNOREtQCGeXQ3LCUVjoFnA6/ooNS7VAc8M686reztWim8AgyLLVolUJZGbRuXfXrioGoX9UHRRhKK5UCrltbwLFLCC/4Fx3kp4kwsTtIpdwsmik/tG6tTXeCoGiVwubN6qwxlChf1eeDfJlB0ingKcvW8LdJX6R8jx/RQX6ZCFOVht8Zi2YKnt12g/LyYD47ckpBRHoBjwK1gaedcwODOE55udYQMYqX5Ctege329qDNIKkUcKZILz+ig/wyEaYyQ+2MRTMFT716emEbBJFys4pIbeBxoDdwMNq68+AgjlVe7k+jCiOeJIeGAuxsuMl3Oe5MV9cbvUzwmuBXYbmqdgEWzZQf6tcvnp3CUcBC59xiABEZBvRB+zb7yubN/vVQMOLFqOll/HLETLZV0XUw0wJYpblp3jzYsAG6dctKpkwO6O82bvFl5+KHiTCTnCVF4ouKAkWzUwBKgC+THi/3xnZARK4RkSkiMmX16upFTwSVDWhEm8QOoSqFAOnNIFn1i1i3Dr75Jmu5UiWpJROVRkLpkun+fHEXPhjQwxRCnhAJrqdC1JRCVjjnhjjnSp1zpc2aNavWZwSpaY3oko1NHDKbQbLKAD/qKG3plyXJ5p10hOHA3blXBGD9DSLA5s3Bmb+jZj4qA5IDRVt5Y74TpE3OiC7ZLqy71U1/vZRLeGcuUU0J8066ZvX5duBaP+xKopakF6RPNGo7hclABxFpJyL1gH7AmCAOZDuF4iTdwip4+QIeCTt+KgdvtjWEUpmZbh0+g3tHfZpRxqh077N+2EqQ7WWrS5A+0UgpBefcVuAmYBwwFxjhnAuk8ZztFAqfVG0y0y24jXevu0sNonQLYLaLdros4pcnfZFxQYlKC0qrqqpEUTkGuVOImvkI59xYYGzQx9l3Xw0QMQqTTKaPB88/dBdTwK3DZ6T8nFQLYLYZ4OkWT+e9N9MiH4VkwqB7RcSFKCrHlSu1lXAQRE4p5ItOneCpp8KWIhyiZh8NgkxXd6miZNKVbUi3AGazaGcK34zD1XbUK93miygqxyBrt0XKfJRPkhtVFBNRtI8GQa5Xd0HY8fuf0Yl0Uc9xuNqOihkrbKLi40lm/nwtnx0ERbtT2GcfjfVdvVpNScVCVFoyBr1byfXqLoiigH27ljBl2ZpY9zCIghkrbKJYMDLInULRKgWRykYVxaQUomAfzUfJ5eqYPoJYAH/f91BK92uatwWlGEyDYRAl5bh+PaxZA23aBPP5RasUQDXtvHlw4olhSxI8icUinbUsn+aMfOxWonR1l68FZdT0Mvq/MnN7Ce6ytZvo/8rM7TIYhcHnn8MBBwTXIKzolUJQjSqiRFXljvNtzsjXbiVKV3f54L4xs3fpybClwnHfmNlFNQ+FTtANwopeKXzwQdhSBE+m0g5hFDGLYjRHPgjatLN205acxo14EqSTGUwpFMVOId0VuEAo5QqKMdTRWlcq5vOopLpzMX8+nH12cHIVtVLYf3/48kvYuBF23z1saYIjalfmUbL3+0VVP3A//SjpjtVk97p8t3HXXUGT3etW75/yGVOMlWQzF+nO8+zZcPvtwclW1Eqhfn04+miYOBHOOSdsaYIjilfmhWTvr+oHPmp6mW9JbJmO9ZtzOtP/1Zk7lOuoW1v4zTmdc/6fgiAq4dBRoKq5SHeev1lVixUrWtClS3CyFW3yWoI+fWD06LClCBZLQgqWTD/wxI87Hbnu1qpaTAZdePgO53nQhYdH5jxHIRw6KlQ1F+nO8x8Gr+PMM6FOgJfzRb1TAFUKAwfCtm1QO32Pk9hTSFfmUSPTDzyTk786u7WqFpMon+eomTHDpKq5SHeeV85qSp9BgYpmO4X27TV57eOPw5bEiCuZSmlnugquzm4t27LdUSSK5SLCoqq5SHU+K8rrsHlFk1x6N1WLolcKAOeeC2MC6dpgFAOZfuDpFuuSxg2qdUWfj4U1VclxPzAzZiVVzUWq87zti+YcXrqVPfYIVraiNx+BmpCuuELNSIaRK1VFU/np5A86civoCKEom7fyTaa5SHWe62zqyCWX7Ra4XOJiXia0tLTUTZkypUafUVEBrVrBu+9Cx47+yGUYCeIUm5+uFWhJ4wZF14IzSmzZAs2bw2efQcuW/nymiEx1zpXuPB7ITkFEBgHnAJuBRcBPnXNrRaQt2lEtkTI2yTl3nfeebsDzQAO0yc4tLk8aq1YtNSGNHg39++fjiEYxEaerY4sQiibvv6/1jvxSCJkIyqcwHjjEOXcY8DlwV9Jzi5xzXbzbdUnjg4GrgQ7erVdAsqWkGEJTjcLDb/t/nB3ZhcyYMbpG5YNAlIJz7l9ev2WASUCrTK8XkRbAns65Sd7u4EWgbxCypaNHD1iwAObOzedRjbgQlPO1pjL53TAp3xFCUZzXqLFxIwwbBj/+cX6Ol4/oo6uAt5IetxOR6SLynogkilaXAMuTXrPcG0uJiFwjIlNEZMrq1at9EbJ+fbjpJhgUcAywET+i2q2uqoby1Vlw8xkhFNV5jRrPPgvHHZc/f2e1Hc0iMgH4UYqn7nHOjfZecw9QCpzvnHMiUv4ilWIAABlwSURBVB9o5Jz71vMhjAI6Ax2Bgc65nt77TgTudM5VWfbJD0dzgjVr1G43cya0bu3LRxY8cXKiVpegna/VncN2A95M2R9DgEcu7pIy6ilKIaDm1K6aLVugQwfdKRxzjL+f7bujObGAZzjglcDZwKkJh7Fzrhwo9+5PFZFFqEIoY0cTUytvLK80bQo//Sk88gj86U/5Pnr8KJYCZ0E6X2syh5myYuNQZ6iYnNrVVfzDh0O7dv4rhEwEYj4SkV7AHcC5zrmNSePNRKS2d7896lBe7JxbCawTkWNERIDLgVDcvrfeCs8/r7uGYiNXc0NV5ot8EbRdOkjna03mMJP9P93CWrZ2U2Rs98Xi1K6umcw5eOghGDAgP3ImCMqn8FdgD2C8iMwQkSe88ZOAWSIyA3gVuM45l1h+bwCeBhaiYaxvEQKtWkHfvvD442EcPTyq88WNwpVePuzSQTpfazKHmez/mRbWqNjui6XsRXUV/9ixWvju9NODlG5XAslTcM4dkGb8NeC1NM9NAQ4JQp5c6d8fTj4ZbrsNGjYMW5r8UB1zQxQKnMW933NN5zBdDkSqcunJRMGUVIh9NVJRXcU/cCDceSeIBCFVeqzMRQoOOghOOEG9/jffHLY0+aE6X9wo9GmIe7/noOYwecH1q5dDEMQpsa+6VEfx/+c/sGIFXHhhkJKlxgripeHOO+GPf1TvfzFQHftuFAqcxd0uHeQc9u1awgcDelAS8zmKO9Uxkz30kFosguybkA7bKaTh6KM1FOzJJzV/odCp7hVr2Fd6Udit1JSg57AQ5ijO5Gom++ADmDYNRozIp5SVWEG8DMydCyedBNOnqwO60IlrzkFc5c4nNkfxoLwcunaF3/42eNNRujwFUwpVcP/9MHWq1kXKt8OnkLFFyjB25f77dZcwalTw601eq6QWEgMGqOZ+9dX81R4pVBKKoGztJgS2Z+MWatKbYeTCnDnw2GMwY0a4F6DmaK6C+vXh6afhllvgu+/Clia+JOcTALuUZwgj6c0wokJFBVx9te4UwjZVm1LIguOOg/POgzvuCFuS+JKpgX2CsrWbfE2osgqc8aHYz9WTT2oG8/XXhy2JmY+y5sEHoXNn7c7WvXvY0sSPbGPi/TIjFUtdpkKg2M/V8uXw61/r2lIrApfpERAhHuy5p5a+uOYa2BR+zk/syDYm3i8zUlTqMhlVU8znyjkNeb/hBr3ojAKmFHLg3HPh8MPh978PW5L4kSqBJx1+ZNrmI9O52E0efhGFGlphMXIkzJ8Pd98dtiSVmFLIkcceg2eegYkTw5YkXqTK3G3coG7K1/qRaRt0prM1iPGPuGelV5clS+DGG+GppzSgJSqYUsiRH/1IG1785CewcGHY0sSLRNmFJQPP4oMBPbjv3M6BVckMugJnMZs8/KZYqqUms369Wh7uvlvrrEUJczRXg+7dNXTs3HNh0iT1N9SUYkzmCrJKZtAVOIvZ5OE3uZyrQvidVFTAZZdp45woFty0jOYacMMN8MUXmu1cOztzeUp2jr6A6LVONHbEWknmn0L5ndx7L7z3npqg69ULT450Gc2BmY9E5D4RKfOa7MwQkTOTnrtLRBaKyHwROSNpvJc3tlBE8txvKHcefRQ2bIB77qnZ5+RiijDnZjQoRpNH2BSCyW74cPjb3+C118JVCJkI2nz0iHPuj8kDInIw0A/oDLQEJohIR+/px4HTgOXAZBEZ45ybE7CM1aZuXXjlFTjqKDjkEPif/6ne52Rriij2eO4o0bdrCVOWrWHox1+yzTlqi3BBt8LvDRAmcTfZTZ2q4afjx8O++4YtTXrCcDT3AYY558qdc0vQ9ptHebeFzrnFzrnNwDDvtZFmn31gzBjt7fzJJ9X7jGyjLwrhSqlQGDW9jNemlrHNM79uc47XppbZzi1A4hyl9NVXWhXhiSegS5ewpclM0ErhJhGZJSLPikgTb6wE+DLpNcu9sXTjuyAi14jIFBGZsnr16iDkzolDDtH6SOefr92SciVbU0Tcr5QKCVPQ+SeuJrvyclUIP/sZXHBB2NJUTY2UgohMEJHPUtz6AIOB/YEuwErgYR/kBcA5N8Q5V+qcK23WrJlfH1sj+vTRuiV9+sC6dbm9N9vuW3G+Uio0TEHnnyh0+suVbdtUGZSUwK9+FbY02VEjn4Jzrmc2rxORp4A3vIdlQOukp1t5Y2QYjwV33w1lZXDaafDPf0KTJlW/J0E23besg1Z0qE7fXaPmhN3pLxe2boUrr9Q14Y03olHXKBuCjD5qkfTwPOAz7/4YoJ+I1BeRdkAH4BNgMtBBRNqJSD3UGT0mKPmCQETrIx17LPTsCd9+6+/nx/FKqVCJqymjJljkW/Zs2QKXXgpffw1vvgkNG4YtUfYEGX30vyLSBS2dvxS4FsA5N1tERgBzgK3Ajc65bQAichMwDqgNPOucmx2gfIEgAo88os15evTwP9IgTldKhUzQyXFRwyLfsmfzZujXT/+OGQO77Ra2RLlhyWsB4ZyWwx05EiZMgBYtqn5PVCmELFKjZliyXnb897/aW7luXc1JiGouAlg7zrwjAr/7nX4puneHt99WZ1PcsCtEA8yxng2bNkHfvrDXXvDyy6oY4khMXB/x5Ve/0uiDk0/Wkhhxw0IvDbDIt6rYsAHOOguaNYO//z2+CgFMKeSFO+7QTMaTT4bFi8OWJjfsCtGA4nSsZ8v69dC7N+y3H7zwAtSJuf0l5uLHh1/8Qmumd+8OY8dqwlscsNBLA4rPsZ4tX3+tuUmHHw7/93/xCTvNhCmFPHL99Vpm+5RTNHT1oovClqhqLDfCSGCRbzsyebI6lS+7TP2HImFL5A+mFPLMpZfCQQdpuvuUKfCHP0R7u2lXiIaxK88+q2HnTz6pJSwKCQtJDYlvv9VYZue0k9s++4QtkWEYVVFeDrfcAu++C6NGwYEHhi1R9cl7PwUjM3vvraUwSkv1Nm1a2BIZhpGJFSvUJ7hqlVZEjrNCyIQphRCpXRsGDoQ//hHOOANefDFsiQzDSMV//gNHHglnn60NcvxowRtVImzNLh4uvFD9DOedp1cgf/pTtDMhDaNYcE6DQn73O3j+eQ09LXRspxAROndWhbBsGZx6qjblMAwjPDZt0iqnQ4bAhx8Wh0IAUwqRonFjGD1aK6x27Qqvvx62RNHCqnQa+WLKFDUXbdkCH30E++8ftkT5w5RCxKhVC37zG3j1VbjzTg1h9bsEdxxJ1GAqW7sJR2UNJlMMhp9s3qylac46S/ujvPxyvMpe+4EphYhy/PEwY4aW3T7sMC3BW8xYDSYjaKZP10jAmTP1t/eTnxROQloumFKIMLvvrr0Zhg6FW2+Fyy8v3l1DodZgMpNY+JSXw333aQTg7berCTfOpe5riimFGHDSSTBrFjRtqg7pl17SqIhiohCrdJpJLHzee0/rFk2frrfLLy/O3UEygSgFERkuIjO821IRmeGNtxWRTUnPPZH0nm4i8qmILBSRv4gU+6nZkYYN4c9/1l6vf/qT9oFeuDBsqfJHIVbpNJNYeKxZoyXtL70UHnxQdwdx7HcSBIEoBefcxc65Ls65LsBrwMikpxclnnPOXZc0Phi4Gu3Z3AHoFYRscae0VAtx9e4NxxwDDzygzrFCpxD7UxeqSSzKOKfO486d1Tw7Z07h1S6qKYEmr3lX+xcBGfv1iUgLYE/n3CTv8YtAX+CtIOWLK3XqwC9/qUX1brpJE99+9zutpVQIpXvTUWhVOq0seX557z0tYvff/2rdoqOPDluiaBL0EnIisMo5tyBprJ2ITBeR90TkRG+sBFie9Jrl3lhKROQaEZkiIlNWr17tv9QxoW1bNSc9/TQ8+igccQS89Vbx+RviSiGaxKLIjBm6s/7pT/UiaupUUwiZqLZSEJEJIvJZilufpJddAgxNerwSaOOc6wrcBvxdRHKuIuKcG+KcK3XOlTZr1qy6/0LBcMopMGmS5jfcdpsW7froo7ClMqqiEE1iUWLRIg0r7d1b8w7mzVMfQiHvpv2g2uYj51zPTM+LSB3gfKBb0nvKgXLv/lQRWQR0BMqAVklvb+WNGVkiorbRc87RwnoXX6w7hwceUPupEU0KzSQWBb76Ss2pw4drx8MhQ6BRo7Clig9B6syewDzn3HazkIg0E5Ha3v32qEN5sXNuJbBORI7x/BCXA6MDlK1gqVMHrroKPv9cQ1lPOUXrtyxbFrZkhhEs338P99yjF0G77aY7g3vvNYWQK0EqhX7saDoCOAmY5YWovgpc55xb4z13A/A0sBBYhDmZa8Ruu6kpacECaN1adw233gpF7IIxCpRNm7T8fIcOukuYPh0eftgaV1UX67xWJKxaBb//Pfz972pXvflm/RHFjVHTy6w1qAHoBc6TT8Lgweo4fuABjcQzssM6rxU5zZvDY49pXZdGjbS20tlnw/jx8YlWsgxgA/Q7fNVV0LEjLF2qEXcjR5pC8AtTCkVGq1bwhz+oj6FvXzUxde4MTzwBGzaELV1mLAO4eNm2TRf+7t01kqhDBzWNPv20Fow0/MOUQpHSoAH8/OdaU+nxx7Vf9H77wR13RNcpbRnA0SWown7ffaf+gv33Vz/B9dfDkiVw113mMwgKUwpFjohGKI0apZ3ftm5Vp/SFF8K//x0t01KciuIVU/XTIMx68+bBDTeoMpg5E155BT74QEOt69b1T3ZjV0wpGNtp316L7S1bpori5z+Hbt3UtPTNN2FLF58M4GLzffhl1tu4EUaMgF691EzUrBnMnq1VgY880keBjYyYUjB2oVEjuPFGmDtX/Q/vvQcHHABnnqmJcevW1fwY1bmSjksGcLH5Pmpi1tu8WUu1XHoptGwJzzyj95ctg/vvL+6+BmERaEE8I97UqqVXbb16wQ8/wD/+oQ1/br5ZS3f366dOvwY5Wm8SV9KJhTNxJQ1UucDHIQO42HwfuRb227ZNLzSGDtU+5AcdBJdcog2l9t03aGmNqrCdgpEVjRrpD3fMGHX09e6tZqWWLbUxydix2uQ8Gwr9SjpOvg8/yMas55zW57rlFo2Au/12DSmdNk19VzfcYAohKphSMHKmaVNtUDJhgpqYSks1Ma5lS7juOnj3XaioSP/+Qr+Sjovvwy/SmfX6dClh1iyNFGrfXsutNG2q349p06B/f2jTJmThjV2wjGbDN5YuhWHD1CywYgWceqqamU47bccf//ED305pbihp3IAPBmRsvREbijXz+ttv4e23NSly/Hi9OOjXT29duliryyiRLqPZlIIRCF98oTuJ8eNh4kRo0qRSQfzQZAW/Hz9rBxNSg7q1I+k0NjJTXq6hogklsGABnHgi9Oyp5/rgg00RRBVTCkZoVFRorHlCSXz0EZS0L2fDPivY2mIlbQ8q584zO5pCiAHOwaefViqBDz/UhT+h8I85BurVC1tKIxtMKRiRYdOmHa8uFy2Ck0/WReXYY+HQQ6F+/bClNEAjhT7/HD7+WJX6hAmwxx6VO4FTTtFdoBE/TCkYkWX1ajUxTZigWdULF8KBB2pmdbdu+veww3IPfTVyY+tWDRyYNk1bVk6bpju85s01mCDhI2rbNmxJDT8wpWDEhk2btCZTYnGaOhXmz9cEuoSS6NYNDj8cGjYMW9p4snkzzJlTufhPnapmoUTvjcQ8d+0KjRuHLa0RBOmUgiWvGZGjQQOtj5/cXL28XBetxAL24otaAqFdO128OnfWsMd27fS2997m4ATNPl+ypPI2b57OYfLcdeumNYW6dIE9c+6YbhQaNVIKIvJj4D7gIOAo59yUpOfuAn4GbAP+n3NunDfeC3gUqA087Zwb6I23A4YBewNTgcucc5trIp9RONSvryaM0qTrms2bdXFL7CSmTNGFb/FiNYUkK4l27Soft21bODuMzZu1JETywr94ceX9TZt2nINDDtFkwy5dCmcODH+pkflIRA4CKoAngdsTSkFEDkZbcR4FtAQmAB29t30OnAYsByYDlzjn5ojICGCkc26YiDwBzHTODa5KBjMfGalYuzb1IrlkieZT7Lln5UK5zz7qLG3cOP3fPfbQsh9B4pz2tFi7VktGp/u7Zk2lIli1CkpKdlR6yUqwWTPbMRmpCcR85Jyb6334zk/1AYY558qBJSKyEFUQAAudc4u99w0D+ojIXKAH8BPvNS+gO5AqlYJhpKJxY7WHd+2663MVFdrLN6Egvv1WF9wvv1QTVaqFeONGVSTJymKvvbSMc+3aqjBq16681aqli/G2bXqrqKi8n7itW7frserVqzxGKuW03356ld+mjS76rVpBHTMCGz4S1NepBJiU9Hi5Nwbw5U7jR6Mmo7XOua0pXr8LInINcA1AG8uTN3KkVi0tydGypbYlzYatW+H773dcwNeu1XpPOy/4icfO7aookh/vtdeOC37jxhbjb4RPlUpBRCYAP0rx1D3OudH+i1Q1zrkhwBBQ81EYMhjFRZ066rzee++wJTGMYKlSKTjnelbjc8uA1kmPW3ljpBn/FmgsInW83ULy6w3DMIw8EZTrbAzQT0Tqe1FFHYBPUMdyBxFpJyL1gH7AGKfe7neAC733XwGEsgsxDMMoZmqkFETkPBFZDhwLvCki4wCcc7OBEcAc4J/Ajc65bd4u4CZgHDAXGOG9FuBO4DbPKb038ExNZDMMwzByxzKaDcMwipB0IanWZMcwDMPYjikFwzAMYzumFAzDMIztmFIwDMMwthN7R7OIrAaWVfPt+wDf+CiOX5hcuWFy5YbJlRuFKtd+zrlmOw/GXinUBBGZksr7HjYmV26YXLlhcuVGscll5iPDMAxjO6YUDMMwjO0Uu1IYErYAaTC5csPkyg2TKzeKSq6i9ikYhmEYO1LsOwXDMAwjCVMKhmEYxnYKVimIyI9FZLaIVIhI6U7P3SUiC0VkvoickTTeyxtbKCIDksbbicjH3vhwr+y3HzIOF5EZ3m2piMzwxtuKyKak555Iek83EfnUk+UvkqIXqg9y3SciZUnHPzPpuZzmzme5BonIPBGZJSKvi0hjbzzU+Uoja+Dzkea4rUXkHRGZ433/b/HGcz6nAcm31DsfM0Qk0dO9qYiMF5EF3t8m3rh452yhd86PCEimTknzMkNE1onIL8KYMxF5VkS+FpHPksZynh8RucJ7/QIRuSInIZxzBXkDDgI6Ae8CpUnjBwMzgfpAO2ARUNu7LQLaA/W81xzsvWcE0M+7/wRwfQDyPgz82rvfFvgszes+AY4BBHgL6B2ALPcBt6cYz3nufJbrdKCOd/8h4KEozFeKY+ZlPtIcuwVwhHd/D+Bz77zldE4DlG8psM9OY/8LDPDuD0g6r2d650y8c/hxns7dV8B+YcwZcBJwRPL3Odf5AZoCi72/Tbz7TbKVoWB3Cs65uc65+Sme6gMMc86VO+eWAAuBo7zbQufcYufcZmAY0Me7suwBvOq9/wWgr5+yese4CBhaxetaAHs65yY5Pfsv+i1LFeQ0d34f3Dn3L1fZx3sS2qEvLSHOV17mIxXOuZXOuWne/fVo35K0/c5Jf07zSR/0dwU7/r76AC86ZRLanbFFwLKcCixyzmWqkhDYnDnn3gfWpDheLvNzBjDeObfGOfcdMB7ola0MBasUMlACfJn0eLk3lm58b2Bt0mKUGPeTE4FVzrkFSWPtRGS6iLwnIicmyb48hYxBcJO3JX02sV0l97kLkqvQq6QEYc9XMmHMxy6ISFugK/CxN5TLOQ0KB/xLRKaKyDXeWHPn3Erv/ldA85BkA+0GmXxxFoU5y3V+aiRfrJWCiEwQkc9S3PJyVZYNWcp4CTt+EVcCbZxzXYHbgL+LyJ55lGswsD/QxZPlYT+PXQO5Eq+5B9gKvOwNBT5fcUNEGgGvAb9wzq0jxHO6Eyc4544AegM3ishJyU96O7pQ4uRFfYXnAq94Q1GZs+3kY37qBPnhQeOc61mNt5UBrZMet/LGSDP+Lbotq+PtFpJfX2MZRaQOcD7QLek95UC5d3+qiCwCOnrHTTaZ5CRLLnIlyfcU8Ib3MNe5810uEbkSOBs41fuB5GW+ciTTPAWOiNRFFcLLzrmRAM65VUnPZ3tOfcc5V+b9/VpEXkfNLqtEpIVzbqVn/vg6DNlQRTUtMVdRmTNyn58yoPtO4+9me7BY7xSqyRign4jUF5F2QAfUGTkZ6CAaaVQP3UaO8Raed4ALvfdfAYz2UZ6ewDzn3HYzh4g0E5Ha3v32noyLvS3kOhE5xvNDXO6zLInjJ9ttzwMSkRA5zV0AcvUC7gDOdc5tTBoPdb5SkJf5SIX3fz4DzHXO/SlpPNdzGoRsDUVkj8R9NHDgM0+GRIRM8u9rDHC5F2VzDPB9khklCHbYsUdhzpKOl8v8jANOF5EmnsnrdG8sO/zwmEfxhp7E5egV5CpgXNJz96ARA/NJikZBvfmfe8/dkzTeHj3pC9GtZX0f5XweuG6nsQuA2cAMYBpwTtJzpeiXcxHwV7ysdJ/n7iXgU2CW98VrUd2581muhaitdIZ3eyIK85VG1sDnI81xT0DNC7OS5unM6pzTAGRrj0btzPTO1z3e+N7ARGABMAFo6o0L8Lgn26ckRREGIFtD1CqwV9JY3ucMVUorgS3o+vWz6swP6nNb6N1+mosMVubCMAzD2E4xmo8MwzCMNJhSMAzDMLZjSsEwDMPYjikFwzAMYzumFAzDMIztmFIwDMMwtmNKwTAMw9jO/weya2brcf5tQgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["List of user distance is: [766.8014391781306, 388.0368533314029, 370.5900548345036, 256.20749548712456, 91.38148388397347, 580.29559882195, 270.7817129217859, 106.07962295479197, 581.623118602567, 320.7194729108959, 106.78629868796105, 790.3614071065638, 170.280148569097, 653.42855022676, 489.44799144210816, 88.30956425239957, 396.23117118063783, 948.2070506766682, 347.4324220591593, 258.3845480740348, 252.02049305642512, 367.396659294187, 420.56450678881896, 527.0336449212236, 354.7365955603569, 675.2359810659123, 873.794461900463, 859.1430404965746, 125.18928482587519, 469.68869149617285, 414.78463456905035, 374.1542657571827, 528.1712424697166, 684.7030807003912, 681.4902127837347, 811.9699021981764, 698.363322953956, 809.8370370403147, 325.2625053943605, 643.472605370806, 170.36404133190024, 309.06133736594126, 835.815180817974, 774.1808226620224, 441.57665335333456, 525.4455909333711, 859.1331828196701, 417.6171083339324, 554.182275095128, 126.38941000587923, 148.40308771354273, 159.10167363939848, 586.832564831103, 613.8489792099257, 378.38069515403436, 211.32426906833112, 128.31070929696216, 624.1416632034609, 54.440087017402014, 544.429254761213, 894.7673532597562, 852.1187076211288, 532.8745643906628, 539.446260463497, 550.4429289918785, 763.2482336200347, 395.062316937884, 460.83347852152104, 959.9042415053673, 750.6905521189711, 277.08365403662617, 873.1748132329498, 174.30621678897907, 507.8736234085242, 441.41277507380653, 865.7623647233245, 729.1341248159478, 181.2598698987778, 298.6352199005645, 159.7231765766184, 57.47746035787649, 947.8931031392349, 66.53280507298652, 653.6216309703876, 315.27376063666907, 394.46926811238677, 358.1835179791118, 227.5078452533941, 741.1131802116212, 654.9925222731257, 236.17021677862516, 382.20552024645855, 245.89782340819485, 559.730066241732, 927.3205186811206, 492.4741750863857, 921.630104914461, 109.6038788722554, 973.7623737655708, 337.86459137833964]\n","List of user distance is: [766.8014391781306, 388.0368533314029, 370.5900548345036, 256.20749548712456, 91.38148388397347, 580.29559882195, 270.7817129217859, 106.07962295479197, 581.623118602567, 320.7194729108959, 106.78629868796105, 790.3614071065638, 170.280148569097, 653.42855022676, 489.44799144210816, 88.30956425239957, 396.23117118063783, 948.2070506766682, 347.4324220591593, 258.3845480740348, 252.02049305642512, 367.396659294187, 420.56450678881896, 527.0336449212236, 354.7365955603569, 675.2359810659123, 873.794461900463, 859.1430404965746, 125.18928482587519, 469.68869149617285, 414.78463456905035, 374.1542657571827, 528.1712424697166, 684.7030807003912, 681.4902127837347, 811.9699021981764, 698.363322953956, 809.8370370403147, 325.2625053943605, 643.472605370806, 170.36404133190024, 309.06133736594126, 835.815180817974, 774.1808226620224, 441.57665335333456, 525.4455909333711, 859.1331828196701, 417.6171083339324, 554.182275095128, 126.38941000587923, 148.40308771354273, 159.10167363939848, 586.832564831103, 613.8489792099257, 378.38069515403436, 211.32426906833112, 128.31070929696216, 624.1416632034609, 54.440087017402014, 544.429254761213, 894.7673532597562, 852.1187076211288, 532.8745643906628, 539.446260463497, 550.4429289918785, 763.2482336200347, 395.062316937884, 460.83347852152104, 959.9042415053673, 750.6905521189711, 277.08365403662617, 873.1748132329498, 174.30621678897907, 507.8736234085242, 441.41277507380653, 865.7623647233245, 729.1341248159478, 181.2598698987778, 298.6352199005645, 159.7231765766184, 57.47746035787649, 947.8931031392349, 66.53280507298652, 653.6216309703876, 315.27376063666907, 394.46926811238677, 358.1835179791118, 227.5078452533941, 741.1131802116212, 654.9925222731257, 236.17021677862516, 382.20552024645855, 245.89782340819485, 559.730066241732, 927.3205186811206, 492.4741750863857, 921.630104914461, 109.6038788722554, 973.7623737655708, 337.86459137833964]\n","List of user path loss is: [246.56786314 235.71163815 234.97840101 229.0953202  212.66349616\n"," 242.12592846 229.97712945 215.0406932  242.16234895 232.67479959\n"," 215.14652009 247.05020413 222.58371955 244.0177705  239.41223041\n"," 212.11848412 236.04471502 249.95234672 233.94994162 229.23018196\n"," 228.83269595 234.84046195 236.99465708 240.5914671  234.28154997\n"," 244.54101965 248.64972086 248.38020299 217.68071471 238.75543082\n"," 236.77409162 235.13096073 240.62583337 244.76293375 244.68796817\n"," 247.48011548 245.07778822 247.4381932  232.89898791 243.77305329\n"," 222.59157017 232.08463974 247.94144636 246.72051642 237.77172401\n"," 240.54336851 248.38002011 236.88256295 241.39205157 217.8327822\n"," 220.39195579 221.50146326 242.30447136 243.02185844 235.30999323\n"," 226.02564208 218.07324864 243.28689248 204.40822035 241.10905136\n"," 249.02776177 248.24935367 240.76713716 240.96249846 241.2841413\n"," 246.49383515 235.99762776 238.45206561 250.14776432 246.22941687\n"," 230.34382026 248.63841404 222.95618258 240.00123266 237.76580776\n"," 248.50253239 245.76503248 223.57967107 231.53767563 221.56360336\n"," 205.27356084 249.94706864 207.60541511 244.02247948 232.40184326\n"," 235.9736835  234.43567543 227.20176704 246.02476251 244.05587375\n"," 227.79736221 235.47029826 228.44069692 241.55081594 249.59733621\n"," 239.51047303 249.49922915 215.56161141 250.37622471 233.50485634]\n","List of user channel gain is: [246.56786314 235.71163815 234.97840101 229.0953202  212.66349616\n"," 242.12592846 229.97712945 215.0406932  242.16234895 232.67479959\n"," 215.14652009 247.05020413 222.58371955 244.0177705  239.41223041\n"," 212.11848412 236.04471502 249.95234672 233.94994162 229.23018196\n"," 228.83269595 234.84046195 236.99465708 240.5914671  234.28154997\n"," 244.54101965 248.64972086 248.38020299 217.68071471 238.75543082\n"," 236.77409162 235.13096073 240.62583337 244.76293375 244.68796817\n"," 247.48011548 245.07778822 247.4381932  232.89898791 243.77305329\n"," 222.59157017 232.08463974 247.94144636 246.72051642 237.77172401\n"," 240.54336851 248.38002011 236.88256295 241.39205157 217.8327822\n"," 220.39195579 221.50146326 242.30447136 243.02185844 235.30999323\n"," 226.02564208 218.07324864 243.28689248 204.40822035 241.10905136\n"," 249.02776177 248.24935367 240.76713716 240.96249846 241.2841413\n"," 246.49383515 235.99762776 238.45206561 250.14776432 246.22941687\n"," 230.34382026 248.63841404 222.95618258 240.00123266 237.76580776\n"," 248.50253239 245.76503248 223.57967107 231.53767563 221.56360336\n"," 205.27356084 249.94706864 207.60541511 244.02247948 232.40184326\n"," 235.9736835  234.43567543 227.20176704 246.02476251 244.05587375\n"," 227.79736221 235.47029826 228.44069692 241.55081594 249.59733621\n"," 239.51047303 249.49922915 215.56161141 250.37622471 233.50485634]\n","List of user data rate is: [140775.86959635 140775.74127497 140775.7321806  140775.65710521\n"," 140775.42541236 140775.81848324 140775.6686029  140775.46112212\n"," 140775.81890995 140775.70323603 140775.46269349 140775.87503599\n"," 140775.56938198 140775.84048006 140775.78632357 140775.41711251\n"," 140775.74538748 140775.90732201 140775.71932852 140775.65886937\n"," 140775.65366379 140775.73046339 140775.75705293 140775.80038767\n"," 140775.72348477 140775.8465039  140775.89292366 140775.88992573\n"," 140775.49986595 140775.77843005 140775.75435269 140775.73407748\n"," 140775.80079547 140775.84905088 140775.84819099 140775.87986648\n"," 140775.85265665 140775.87939618 140775.70607809 140775.83765391\n"," 140775.56949083 140775.69572826 140775.88503138 140775.87132021\n"," 140775.76652614 140775.79981673 140775.8899237  140775.75568127\n"," 140775.80985749 140775.50206902 140775.53868888 140775.55430215\n"," 140775.82057387 140775.82894308 140775.73630038 140775.61638073\n"," 140775.5055465  140775.83202255 140775.29495302 140775.80651717\n"," 140775.89711778 140775.88846791 140775.802471   140775.80478429\n"," 140775.80858473 140775.8687596  140775.7448068  140775.77476948\n"," 140775.90946909 140775.86576676 140775.67335817 140775.89279802\n"," 140775.5745379  140775.79336555 140775.76645425 140775.89128725\n"," 140775.860495   140775.58313027 140775.68873584 140775.55517198\n"," 140775.30912042 140775.90726397 140775.34670973 140775.84053439\n"," 140775.69976833 140775.74451143 140775.72541252 140775.63211406\n"," 140775.86344595 140775.84091959 140775.64001952 140775.73828788\n"," 140775.64851233 140775.811728   140775.90341286 140775.78750055\n"," 140775.90233061 140775.46884205 140775.91197496 140775.71373144]\n"]}]},{"cell_type":"code","metadata":{"id":"7ROqWdaFwwZb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636347045075,"user_tz":-540,"elapsed":575,"user":{"displayName":"Dương Minh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17555676595979985239"}},"outputId":"33cdea1e-7d65-499f-9329-49e4ca75eda5"},"source":["print(len(federated_train_data_for_iid))\n","print(len(federated_test_data_for_iid[0]))"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["100\n","10000\n"]}]},{"cell_type":"markdown","metadata":{"id":"-uAVOm8ovDD6"},"source":["## FL Process"]},{"cell_type":"code","metadata":{"id":"muNCGvNErU8s","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1636347974838,"user_tz":-540,"elapsed":20249,"user":{"displayName":"Dương Minh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17555676595979985239"}},"outputId":"6d68b70b-5bea-4a40-9a62-cbb8a1c17478"},"source":["for round in range(TRAINING_ROUNDS) :\n","# Client Side\n","  # Train client\n","  sent_clientModel = FL_Env._clientTrain(get_serverModel)    \n","  # Update Client2Server\n","  # Client2Server Propagating\n","  get_clientModel, get_clientSize, get_clientAccuracy, get_clientLoss = FL_Env.client2server_env(sent_clientModel)  \n","\n","  ########### ########### ###########\n","  # Server retrieval\n","\n","  # Server Aggregation        \n","  sent_serverModel = FL_Env._ServerAggregate(get_clientModel, get_clientSize, get_clientAccuracy, get_clientLoss)   \n","  # Update Server2Client      \n","  # Server2Client Propagating\n","  get_serverModel = FL_Env.server2client_env(sent_serverModel)"],"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","▶ Round 1 ◀\n","=====================\n","train label is: (600,)\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/backend.py:4907: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n","  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"]},{"output_type":"stream","name":"stdout","text":["19/19 [==============================] - 16s 6ms/step - loss: 1.4610 - accuracy: 0.5050\n","Epoch 2/50\n","19/19 [==============================] - 0s 4ms/step - loss: 0.3884 - accuracy: 0.8717\n","Epoch 3/50\n","19/19 [==============================] - 0s 4ms/step - loss: 0.2094 - accuracy: 0.9300\n","Epoch 4/50\n","19/19 [==============================] - 0s 4ms/step - loss: 0.0712 - accuracy: 0.9833\n","Epoch 5/50\n","19/19 [==============================] - 0s 4ms/step - loss: 0.0398 - accuracy: 0.9917\n","Epoch 6/50\n","19/19 [==============================] - 0s 18ms/step - loss: 0.0117 - accuracy: 0.9983\n","Epoch 7/50\n","19/19 [==============================] - 0s 3ms/step - loss: 0.0129 - accuracy: 0.9983\n","Epoch 8/50\n","19/19 [==============================] - 0s 3ms/step - loss: 0.0104 - accuracy: 0.9983\n","Epoch 9/50\n","19/19 [==============================] - 0s 4ms/step - loss: 0.0217 - accuracy: 0.9917\n","Epoch 10/50\n","19/19 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.9933\n","Epoch 11/50\n","19/19 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 0.9983\n","Epoch 12/50\n","19/19 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000\n","Epoch 13/50\n","19/19 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n","Epoch 14/50\n","19/19 [==============================] - 0s 4ms/step - loss: 5.3327e-04 - accuracy: 1.0000\n","Epoch 15/50\n","19/19 [==============================] - 0s 4ms/step - loss: 3.1466e-04 - accuracy: 1.0000\n","Epoch 16/50\n","19/19 [==============================] - 0s 4ms/step - loss: 2.5041e-04 - accuracy: 1.0000\n","Epoch 17/50\n","19/19 [==============================] - 0s 4ms/step - loss: 2.1665e-04 - accuracy: 1.0000\n","Epoch 18/50\n","19/19 [==============================] - 0s 4ms/step - loss: 1.9313e-04 - accuracy: 1.0000\n","Epoch 19/50\n","19/19 [==============================] - 0s 4ms/step - loss: 1.8045e-04 - accuracy: 1.0000\n","Epoch 20/50\n","19/19 [==============================] - 0s 4ms/step - loss: 1.5835e-04 - accuracy: 1.0000\n","Epoch 21/50\n","19/19 [==============================] - 0s 4ms/step - loss: 1.4590e-04 - accuracy: 1.0000\n","Epoch 22/50\n","19/19 [==============================] - 0s 4ms/step - loss: 1.3371e-04 - accuracy: 1.0000\n","Epoch 23/50\n","19/19 [==============================] - 0s 4ms/step - loss: 1.2249e-04 - accuracy: 1.0000\n","Epoch 24/50\n","19/19 [==============================] - 0s 4ms/step - loss: 1.1336e-04 - accuracy: 1.0000\n","Epoch 25/50\n","19/19 [==============================] - 0s 4ms/step - loss: 1.0601e-04 - accuracy: 1.0000\n","Epoch 26/50\n","19/19 [==============================] - 0s 3ms/step - loss: 9.8972e-05 - accuracy: 1.0000\n","Epoch 27/50\n","19/19 [==============================] - 0s 4ms/step - loss: 9.1443e-05 - accuracy: 1.0000\n","Epoch 28/50\n","19/19 [==============================] - 0s 3ms/step - loss: 8.7262e-05 - accuracy: 1.0000\n","Epoch 29/50\n","19/19 [==============================] - 0s 4ms/step - loss: 8.1308e-05 - accuracy: 1.0000\n","Epoch 30/50\n","19/19 [==============================] - 0s 4ms/step - loss: 7.6896e-05 - accuracy: 1.0000\n","Epoch 31/50\n","19/19 [==============================] - 0s 4ms/step - loss: 7.2447e-05 - accuracy: 1.0000\n","Epoch 32/50\n","19/19 [==============================] - 0s 4ms/step - loss: 6.8261e-05 - accuracy: 1.0000\n","Epoch 33/50\n","19/19 [==============================] - 0s 4ms/step - loss: 6.5450e-05 - accuracy: 1.0000\n","Epoch 34/50\n","19/19 [==============================] - 0s 3ms/step - loss: 6.2015e-05 - accuracy: 1.0000\n","Epoch 35/50\n","19/19 [==============================] - 0s 4ms/step - loss: 5.8865e-05 - accuracy: 1.0000\n","Epoch 36/50\n","19/19 [==============================] - 0s 4ms/step - loss: 5.5837e-05 - accuracy: 1.0000\n","Epoch 37/50\n","19/19 [==============================] - 0s 4ms/step - loss: 5.3428e-05 - accuracy: 1.0000\n","Epoch 38/50\n","19/19 [==============================] - 0s 4ms/step - loss: 5.0946e-05 - accuracy: 1.0000\n","Epoch 39/50\n","19/19 [==============================] - 0s 3ms/step - loss: 4.8578e-05 - accuracy: 1.0000\n","Epoch 40/50\n","19/19 [==============================] - 0s 3ms/step - loss: 4.6679e-05 - accuracy: 1.0000\n","Epoch 41/50\n","19/19 [==============================] - 0s 3ms/step - loss: 4.4174e-05 - accuracy: 1.0000\n","Epoch 42/50\n","19/19 [==============================] - 0s 4ms/step - loss: 4.2475e-05 - accuracy: 1.0000\n","Epoch 43/50\n","19/19 [==============================] - 0s 3ms/step - loss: 4.0993e-05 - accuracy: 1.0000\n","Epoch 44/50\n","19/19 [==============================] - 0s 4ms/step - loss: 3.8955e-05 - accuracy: 1.0000\n","Epoch 45/50\n","19/19 [==============================] - 0s 4ms/step - loss: 3.8066e-05 - accuracy: 1.0000\n","Epoch 46/50\n","19/19 [==============================] - 0s 4ms/step - loss: 3.6311e-05 - accuracy: 1.0000\n","Epoch 47/50\n","19/19 [==============================] - 0s 4ms/step - loss: 3.4734e-05 - accuracy: 1.0000\n","Epoch 48/50\n","19/19 [==============================] - 0s 4ms/step - loss: 3.3637e-05 - accuracy: 1.0000\n","Epoch 49/50\n","19/19 [==============================] - 0s 4ms/step - loss: 3.2420e-05 - accuracy: 1.0000\n","Epoch 50/50\n","19/19 [==============================] - 0s 4ms/step - loss: 3.1251e-05 - accuracy: 1.0000\n","=====================\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-2b4617df3cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Client Side\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# Train client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0msent_clientModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFL_Env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clientTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_serverModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;31m# Update Client2Server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# Client2Server Propagating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-24ba8ac6dd7c>\u001b[0m in \u001b[0;36m_clientTrain\u001b[0;34m(self, global_params)\u001b[0m\n\u001b[1;32m     95\u001b[0m                                                        self.epochs)\n\u001b[1;32m     96\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=====================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mlist_of_local_parameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregional_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mlist_of_local_dataset_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mlist_of_local_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'FL_Environment' object has no attribute 'regional_model'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z5x0FlND4tRe","executionInfo":{"status":"ok","timestamp":1636347364244,"user_tz":-540,"elapsed":1367,"user":{"displayName":"Dương Minh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17555676595979985239"}},"outputId":"67ffe7ba-5429-40d5-b990-8ffae7b35198"},"source":["mnist_train, mnist_test = tf.keras.datasets.mnist.load_data() # This dataset is not \"E\"mnist. Don't confuse!\n","print(mnist_test[1].shape)\n","raw_dataset_for_iid=list(zip(mnist_train[0].reshape(-1, 28, 28, 1).astype(\"float32\")/255.0, mnist_train[1].astype(\"float32\")))\n","loop_number = 2\n","\n","random.shuffle(raw_dataset_for_iid)\n","\n","el_size=600\n","temp_list_for_image=[]\n","temp_list_for_label=[]\n","federated_train_data_for_iid=[]\n","for idx, el in enumerate(raw_dataset_for_iid) :\n","    temp_list_for_image.append(el[0])\n","    temp_list_for_label.append(el[1])\n","    if (idx+1)%(el_size)==0 :\n","        federated_train_data_for_iid.append((np.array(temp_list_for_image, dtype=\"float32\"), np.array(temp_list_for_label, dtype=\"float32\")))\n","        temp_list_for_image=[]\n","        temp_list_for_label=[]\n","\n","federated_train_data = federated_train_data_for_iid\n","\n","print(\"\\n▶ Round\", round+1, \"◀\")\n","\n","    # check whether to apply shuffle mode per round\n","selected_clients_list = np.random.choice(100, size=10, replace=False)\n","\n","for client_dataset in selected_clients_list :\n","    train_images, train_labels=federated_train_data[client_dataset]\n","    print(train_labels)\n","    "],"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["(10000,)\n","\n","▶ Round 1 ◀\n","[1. 7. 5. 1. 9. 6. 1. 2. 6. 7. 4. 7. 0. 3. 5. 5. 2. 7. 1. 8. 1. 0. 8. 0.\n"," 1. 3. 1. 1. 3. 8. 1. 7. 8. 0. 2. 4. 5. 2. 2. 5. 8. 8. 8. 1. 6. 5. 7. 5.\n"," 6. 1. 5. 2. 7. 0. 1. 4. 5. 3. 7. 2. 4. 1. 8. 3. 7. 5. 6. 6. 3. 4. 2. 1.\n"," 4. 1. 9. 3. 1. 8. 4. 1. 1. 0. 0. 7. 2. 8. 7. 7. 3. 9. 3. 4. 8. 1. 0. 8.\n"," 5. 6. 3. 6. 7. 2. 3. 7. 5. 9. 6. 1. 1. 9. 1. 0. 3. 0. 9. 4. 7. 9. 7. 3.\n"," 1. 7. 3. 9. 4. 3. 8. 2. 4. 2. 0. 8. 8. 0. 4. 6. 4. 0. 1. 9. 1. 9. 1. 0.\n"," 9. 0. 3. 1. 0. 9. 4. 4. 4. 8. 5. 7. 0. 7. 1. 3. 7. 9. 3. 5. 0. 3. 1. 3.\n"," 9. 5. 3. 7. 4. 1. 3. 1. 4. 9. 5. 9. 3. 0. 7. 9. 5. 1. 4. 4. 4. 2. 6. 1.\n"," 3. 8. 8. 8. 2. 0. 4. 2. 4. 8. 6. 3. 0. 6. 5. 6. 5. 6. 8. 3. 3. 3. 0. 4.\n"," 7. 0. 0. 8. 8. 6. 0. 3. 6. 6. 2. 9. 9. 6. 9. 8. 0. 6. 0. 7. 1. 9. 5. 8.\n"," 0. 3. 8. 3. 6. 7. 9. 6. 2. 8. 1. 1. 7. 1. 7. 7. 3. 4. 6. 4. 9. 9. 2. 2.\n"," 0. 5. 8. 2. 7. 9. 3. 8. 4. 7. 2. 9. 2. 2. 1. 1. 4. 5. 5. 4. 6. 4. 1. 1.\n"," 1. 3. 6. 3. 8. 0. 6. 5. 6. 8. 2. 2. 1. 8. 2. 1. 3. 8. 8. 6. 2. 8. 7. 9.\n"," 0. 0. 6. 2. 0. 7. 3. 7. 1. 4. 4. 3. 9. 6. 0. 4. 6. 6. 0. 1. 2. 5. 1. 8.\n"," 3. 4. 5. 9. 4. 6. 4. 0. 8. 8. 8. 9. 8. 4. 8. 4. 0. 1. 9. 7. 0. 6. 5. 1.\n"," 3. 0. 7. 4. 7. 5. 0. 8. 7. 1. 1. 2. 6. 8. 0. 6. 3. 3. 3. 1. 3. 0. 0. 9.\n"," 0. 5. 1. 7. 8. 5. 3. 3. 5. 2. 9. 9. 8. 1. 9. 2. 6. 2. 9. 0. 3. 0. 6. 2.\n"," 2. 2. 3. 4. 9. 7. 7. 1. 5. 3. 4. 3. 6. 9. 2. 1. 7. 9. 5. 3. 5. 4. 1. 3.\n"," 5. 6. 5. 5. 4. 0. 3. 8. 0. 5. 2. 8. 3. 9. 5. 3. 4. 1. 4. 2. 4. 8. 7. 3.\n"," 2. 9. 4. 5. 1. 7. 7. 7. 0. 2. 9. 7. 7. 2. 6. 8. 4. 3. 2. 1. 6. 9. 9. 5.\n"," 7. 0. 7. 1. 9. 1. 4. 1. 7. 6. 0. 3. 4. 0. 0. 5. 2. 8. 2. 1. 5. 8. 5. 7.\n"," 8. 8. 2. 4. 7. 3. 2. 8. 7. 1. 9. 7. 0. 5. 8. 0. 3. 4. 7. 3. 6. 6. 5. 9.\n"," 8. 4. 4. 5. 3. 7. 3. 0. 7. 3. 2. 0. 5. 7. 1. 5. 4. 6. 6. 1. 3. 3. 6. 4.\n"," 1. 4. 0. 1. 4. 0. 2. 3. 7. 1. 3. 1. 1. 9. 6. 9. 5. 5. 9. 6. 3. 8. 2. 3.\n"," 4. 3. 3. 5. 4. 8. 8. 0. 6. 5. 8. 8. 3. 4. 8. 7. 6. 3. 4. 5. 6. 4. 7. 6.]\n","[3. 7. 4. 4. 2. 4. 2. 2. 3. 3. 1. 4. 0. 2. 0. 4. 8. 4. 9. 2. 6. 3. 1. 4.\n"," 6. 8. 7. 0. 9. 2. 6. 3. 1. 0. 1. 9. 2. 9. 0. 1. 8. 1. 9. 5. 4. 5. 0. 1.\n"," 7. 5. 3. 9. 6. 4. 2. 2. 1. 3. 6. 0. 6. 1. 1. 8. 2. 9. 0. 3. 0. 6. 0. 4.\n"," 2. 2. 7. 2. 4. 9. 3. 1. 7. 7. 9. 2. 6. 9. 4. 2. 9. 7. 0. 5. 2. 9. 1. 1.\n"," 5. 6. 8. 0. 0. 3. 0. 4. 1. 8. 0. 1. 8. 4. 5. 6. 5. 8. 6. 7. 9. 0. 1. 4.\n"," 1. 0. 0. 6. 7. 5. 7. 9. 2. 7. 3. 6. 3. 4. 8. 6. 1. 2. 8. 4. 9. 8. 2. 6.\n"," 1. 1. 4. 0. 0. 7. 5. 2. 4. 2. 2. 7. 9. 9. 8. 3. 0. 1. 4. 8. 5. 3. 0. 8.\n"," 1. 2. 8. 2. 8. 0. 7. 1. 3. 5. 8. 3. 7. 5. 4. 8. 8. 7. 9. 3. 5. 0. 4. 5.\n"," 3. 4. 3. 4. 7. 2. 5. 3. 6. 3. 9. 3. 3. 0. 3. 8. 6. 8. 7. 8. 3. 3. 7. 6.\n"," 3. 5. 1. 2. 1. 8. 1. 1. 4. 4. 6. 7. 0. 3. 4. 5. 1. 5. 5. 0. 4. 0. 8. 7.\n"," 1. 5. 3. 0. 0. 5. 2. 8. 7. 7. 4. 2. 2. 5. 6. 1. 6. 0. 1. 3. 1. 4. 7. 3.\n"," 9. 8. 5. 6. 6. 5. 2. 0. 5. 2. 2. 0. 1. 8. 8. 0. 8. 7. 3. 3. 5. 3. 9. 7.\n"," 7. 4. 7. 3. 9. 7. 1. 1. 8. 3. 1. 8. 8. 6. 5. 9. 6. 4. 8. 2. 2. 8. 5. 0.\n"," 4. 4. 3. 4. 5. 4. 1. 3. 0. 2. 7. 5. 5. 6. 9. 3. 4. 9. 0. 0. 4. 7. 3. 6.\n"," 9. 2. 7. 8. 4. 4. 0. 7. 7. 0. 2. 7. 8. 4. 1. 5. 6. 8. 4. 4. 9. 7. 3. 4.\n"," 8. 6. 8. 0. 9. 4. 1. 3. 2. 8. 4. 8. 9. 6. 9. 7. 6. 2. 5. 1. 6. 7. 6. 7.\n"," 5. 3. 6. 7. 1. 8. 4. 6. 0. 6. 1. 9. 0. 6. 1. 4. 8. 3. 1. 5. 4. 1. 5. 0.\n"," 7. 8. 4. 2. 6. 2. 1. 2. 6. 0. 2. 5. 1. 8. 2. 5. 5. 1. 0. 0. 7. 1. 0. 9.\n"," 1. 8. 7. 9. 1. 6. 7. 3. 0. 6. 0. 8. 9. 5. 9. 7. 5. 7. 3. 8. 1. 2. 4. 0.\n"," 7. 4. 7. 2. 3. 0. 5. 4. 8. 6. 3. 5. 6. 0. 1. 4. 7. 4. 0. 3. 4. 9. 4. 0.\n"," 5. 7. 2. 4. 8. 4. 4. 9. 8. 5. 8. 1. 6. 2. 7. 2. 0. 2. 9. 2. 2. 4. 8. 4.\n"," 7. 5. 7. 2. 3. 7. 0. 7. 5. 9. 4. 3. 0. 6. 4. 6. 8. 9. 0. 4. 0. 1. 7. 9.\n"," 2. 3. 2. 9. 7. 8. 8. 6. 0. 5. 8. 5. 3. 0. 4. 5. 0. 5. 4. 9. 9. 8. 3. 1.\n"," 6. 6. 9. 1. 7. 6. 1. 0. 9. 7. 7. 1. 4. 5. 3. 8. 3. 4. 2. 4. 1. 4. 2. 7.\n"," 2. 0. 4. 8. 8. 9. 0. 9. 0. 1. 2. 3. 7. 0. 8. 5. 1. 7. 6. 1. 6. 0. 3. 4.]\n","[2. 8. 6. 8. 8. 9. 2. 6. 3. 6. 3. 8. 1. 4. 1. 0. 6. 0. 0. 2. 1. 6. 7. 2.\n"," 0. 8. 8. 1. 5. 5. 4. 2. 4. 7. 8. 4. 4. 1. 8. 4. 9. 7. 9. 8. 4. 9. 1. 2.\n"," 9. 1. 8. 9. 8. 7. 6. 9. 2. 5. 8. 0. 5. 7. 6. 0. 8. 5. 0. 6. 7. 0. 7. 8.\n"," 5. 9. 7. 0. 3. 9. 8. 5. 3. 1. 1. 6. 5. 6. 8. 6. 3. 8. 7. 8. 7. 5. 6. 2.\n"," 2. 7. 2. 1. 5. 7. 9. 6. 7. 5. 6. 0. 0. 2. 8. 9. 7. 4. 8. 0. 1. 3. 3. 2.\n"," 6. 0. 2. 4. 9. 8. 5. 3. 9. 6. 8. 3. 2. 3. 7. 3. 8. 9. 4. 0. 9. 9. 6. 5.\n"," 8. 6. 3. 7. 9. 8. 2. 4. 3. 0. 8. 1. 0. 7. 9. 2. 0. 0. 1. 1. 4. 6. 4. 0.\n"," 1. 3. 6. 8. 1. 1. 6. 2. 1. 0. 1. 1. 0. 2. 3. 9. 4. 6. 0. 2. 7. 1. 1. 4.\n"," 1. 6. 6. 1. 2. 8. 8. 3. 7. 1. 8. 4. 9. 7. 7. 4. 2. 3. 1. 3. 5. 3. 5. 1.\n"," 9. 8. 7. 3. 3. 0. 8. 5. 3. 6. 4. 7. 3. 9. 4. 7. 5. 1. 6. 6. 3. 2. 8. 5.\n"," 7. 8. 1. 1. 4. 5. 2. 0. 2. 3. 3. 4. 1. 9. 9. 0. 0. 9. 1. 7. 9. 8. 2. 7.\n"," 3. 7. 0. 7. 1. 3. 0. 0. 4. 2. 8. 3. 4. 9. 5. 4. 8. 1. 0. 7. 7. 6. 0. 1.\n"," 6. 4. 9. 7. 1. 4. 6. 0. 5. 6. 4. 8. 1. 0. 2. 3. 0. 9. 0. 4. 4. 7. 1. 3.\n"," 9. 1. 4. 5. 8. 4. 9. 6. 5. 9. 3. 9. 2. 7. 2. 8. 1. 3. 4. 0. 1. 1. 2. 8.\n"," 8. 9. 6. 3. 3. 3. 3. 2. 3. 0. 1. 0. 7. 7. 5. 5. 2. 3. 8. 7. 4. 1. 8. 8.\n"," 3. 6. 8. 2. 6. 3. 1. 9. 2. 9. 8. 8. 3. 8. 1. 6. 4. 6. 2. 0. 3. 9. 6. 5.\n"," 5. 2. 5. 5. 2. 4. 4. 4. 2. 4. 1. 8. 1. 2. 5. 4. 6. 3. 8. 7. 8. 7. 4. 2.\n"," 5. 9. 5. 1. 6. 9. 5. 7. 8. 1. 1. 0. 1. 9. 3. 9. 7. 0. 1. 7. 7. 2. 5. 4.\n"," 2. 5. 7. 4. 2. 5. 7. 8. 0. 1. 0. 1. 0. 3. 4. 2. 7. 3. 4. 8. 5. 1. 8. 0.\n"," 2. 6. 3. 6. 0. 6. 3. 3. 4. 8. 8. 9. 4. 2. 0. 8. 6. 6. 4. 0. 4. 7. 3. 6.\n"," 7. 7. 3. 6. 0. 9. 0. 3. 5. 6. 9. 1. 4. 1. 2. 4. 6. 2. 8. 4. 5. 9. 3. 5.\n"," 4. 0. 0. 6. 0. 6. 4. 6. 6. 8. 2. 9. 9. 4. 9. 8. 2. 6. 6. 6. 8. 1. 9. 4.\n"," 3. 8. 6. 7. 8. 6. 4. 6. 6. 2. 3. 7. 1. 0. 3. 6. 8. 8. 8. 9. 3. 1. 2. 4.\n"," 7. 4. 9. 9. 9. 7. 2. 2. 1. 0. 3. 9. 8. 5. 5. 4. 5. 5. 7. 3. 1. 6. 3. 4.\n"," 2. 9. 0. 7. 4. 4. 0. 0. 1. 6. 4. 3. 9. 3. 4. 7. 0. 0. 4. 8. 3. 0. 1. 1.]\n","[5. 5. 1. 5. 5. 0. 9. 1. 7. 2. 9. 8. 5. 6. 1. 9. 0. 2. 0. 4. 4. 4. 8. 1.\n"," 7. 3. 3. 3. 7. 6. 7. 1. 7. 4. 9. 2. 9. 9. 6. 2. 2. 2. 0. 1. 3. 2. 9. 7.\n"," 6. 3. 0. 5. 3. 5. 4. 7. 2. 4. 1. 1. 6. 7. 5. 6. 3. 1. 9. 5. 4. 8. 3. 1.\n"," 4. 7. 1. 2. 0. 2. 7. 8. 9. 4. 5. 6. 1. 1. 1. 3. 2. 5. 7. 9. 8. 6. 5. 6.\n"," 4. 7. 3. 4. 1. 3. 0. 7. 5. 3. 2. 3. 9. 4. 0. 1. 0. 3. 1. 3. 1. 1. 4. 2.\n"," 5. 9. 6. 4. 5. 1. 3. 0. 9. 2. 0. 8. 6. 8. 8. 8. 3. 7. 0. 1. 2. 4. 7. 2.\n"," 1. 6. 3. 2. 0. 7. 3. 7. 9. 7. 9. 3. 4. 7. 6. 5. 4. 4. 4. 4. 9. 6. 0. 2.\n"," 2. 8. 0. 8. 5. 5. 4. 5. 0. 8. 0. 3. 0. 5. 9. 8. 0. 1. 8. 4. 2. 2. 4. 1.\n"," 2. 4. 9. 6. 4. 8. 9. 2. 3. 5. 3. 2. 6. 6. 8. 7. 3. 9. 1. 1. 6. 7. 6. 4.\n"," 8. 2. 9. 0. 5. 8. 0. 2. 2. 3. 3. 9. 4. 6. 6. 2. 7. 2. 8. 1. 4. 0. 5. 0.\n"," 7. 8. 8. 0. 3. 2. 1. 9. 0. 9. 9. 6. 0. 4. 4. 0. 2. 1. 6. 0. 5. 2. 3. 7.\n"," 1. 0. 0. 8. 9. 1. 0. 8. 2. 6. 6. 7. 0. 6. 1. 5. 8. 0. 8. 4. 4. 4. 2. 4.\n"," 7. 8. 1. 7. 0. 0. 8. 7. 4. 8. 1. 3. 5. 4. 4. 9. 9. 1. 9. 1. 3. 1. 9. 9.\n"," 1. 4. 1. 7. 1. 2. 4. 7. 9. 9. 8. 3. 0. 4. 3. 6. 6. 5. 5. 2. 2. 7. 5. 7.\n"," 1. 3. 4. 9. 1. 4. 2. 7. 8. 1. 9. 2. 1. 4. 3. 4. 9. 6. 5. 1. 7. 2. 8. 5.\n"," 4. 8. 1. 3. 1. 0. 7. 9. 5. 0. 9. 6. 3. 4. 2. 1. 5. 6. 2. 4. 0. 9. 2. 4.\n"," 1. 9. 2. 2. 6. 6. 0. 7. 2. 4. 0. 8. 5. 1. 5. 5. 1. 2. 9. 2. 0. 1. 5. 9.\n"," 0. 3. 5. 7. 1. 2. 3. 9. 1. 6. 4. 1. 3. 5. 3. 9. 9. 9. 6. 5. 4. 5. 3. 3.\n"," 0. 8. 3. 0. 6. 9. 8. 6. 8. 8. 9. 7. 1. 2. 9. 0. 3. 0. 6. 0. 6. 8. 7. 5.\n"," 1. 2. 7. 8. 6. 0. 1. 8. 7. 8. 4. 4. 5. 0. 1. 6. 0. 3. 2. 4. 2. 8. 1. 1.\n"," 0. 9. 3. 3. 7. 5. 8. 3. 6. 1. 1. 8. 7. 5. 9. 3. 5. 7. 5. 6. 4. 8. 0. 8.\n"," 2. 0. 2. 6. 8. 0. 3. 9. 7. 4. 0. 1. 4. 0. 0. 2. 8. 6. 5. 0. 5. 4. 7. 0.\n"," 0. 8. 3. 7. 7. 8. 7. 8. 8. 1. 8. 2. 3. 1. 9. 2. 0. 0. 1. 8. 6. 9. 2. 8.\n"," 9. 1. 7. 0. 6. 2. 9. 6. 7. 3. 9. 5. 4. 1. 2. 1. 2. 4. 0. 4. 6. 0. 4. 7.\n"," 2. 8. 9. 7. 6. 3. 1. 2. 7. 7. 1. 9. 1. 2. 3. 5. 4. 1. 3. 1. 8. 9. 8. 6.]\n","[3. 4. 1. 2. 6. 2. 1. 3. 5. 5. 1. 3. 1. 4. 8. 7. 5. 1. 6. 0. 8. 3. 0. 1.\n"," 9. 6. 8. 9. 1. 3. 2. 4. 4. 0. 0. 5. 1. 0. 9. 8. 0. 0. 1. 0. 8. 2. 2. 3.\n"," 5. 5. 8. 1. 3. 3. 4. 1. 7. 3. 8. 9. 6. 9. 5. 4. 8. 3. 9. 5. 3. 4. 6. 6.\n"," 2. 0. 6. 7. 9. 7. 6. 1. 5. 8. 0. 6. 0. 5. 7. 7. 4. 3. 0. 6. 4. 3. 6. 8.\n"," 8. 4. 1. 0. 5. 7. 3. 5. 3. 1. 8. 3. 3. 4. 4. 2. 5. 6. 8. 4. 3. 4. 8. 1.\n"," 1. 3. 3. 0. 3. 4. 0. 3. 1. 9. 9. 4. 9. 7. 8. 3. 4. 1. 2. 2. 1. 1. 1. 2.\n"," 4. 7. 4. 8. 2. 6. 1. 2. 0. 4. 0. 4. 3. 2. 5. 2. 9. 3. 8. 7. 7. 3. 3. 6.\n"," 9. 3. 9. 6. 2. 4. 1. 1. 7. 7. 5. 3. 5. 1. 1. 8. 9. 1. 0. 2. 7. 1. 0. 4.\n"," 8. 9. 9. 9. 6. 0. 0. 2. 4. 7. 3. 0. 8. 3. 1. 4. 4. 4. 5. 8. 1. 2. 8. 4.\n"," 3. 7. 6. 4. 3. 9. 1. 7. 6. 0. 2. 5. 8. 2. 7. 8. 3. 7. 2. 7. 2. 7. 9. 1.\n"," 3. 7. 7. 3. 5. 7. 6. 2. 8. 7. 2. 7. 9. 7. 6. 2. 1. 8. 5. 1. 7. 3. 7. 7.\n"," 1. 0. 6. 2. 5. 4. 4. 1. 6. 9. 9. 3. 7. 2. 5. 9. 8. 8. 8. 1. 3. 8. 5. 3.\n"," 1. 0. 3. 4. 4. 1. 8. 4. 9. 5. 0. 1. 2. 5. 9. 7. 6. 1. 8. 7. 7. 8. 8. 6.\n"," 6. 0. 5. 3. 3. 3. 9. 0. 0. 4. 5. 9. 7. 3. 7. 9. 8. 7. 8. 4. 9. 9. 6. 3.\n"," 1. 7. 6. 6. 3. 9. 2. 9. 5. 7. 5. 5. 8. 7. 0. 6. 8. 5. 5. 5. 8. 3. 7. 0.\n"," 5. 6. 4. 3. 9. 4. 7. 9. 2. 8. 1. 9. 0. 1. 5. 1. 7. 1. 1. 2. 7. 6. 6. 1.\n"," 2. 4. 1. 0. 6. 6. 8. 4. 1. 6. 5. 4. 3. 7. 2. 5. 1. 1. 5. 5. 2. 5. 8. 9.\n"," 7. 7. 7. 6. 8. 7. 4. 6. 1. 5. 7. 1. 4. 0. 9. 1. 0. 5. 1. 5. 2. 0. 6. 9.\n"," 9. 1. 5. 4. 5. 8. 5. 1. 7. 6. 4. 9. 7. 1. 6. 9. 5. 9. 0. 2. 6. 7. 8. 8.\n"," 1. 1. 3. 4. 2. 5. 8. 5. 4. 7. 0. 7. 5. 6. 0. 1. 0. 4. 8. 1. 6. 2. 8. 5.\n"," 2. 9. 4. 9. 6. 4. 1. 1. 9. 3. 6. 3. 6. 4. 6. 1. 8. 8. 8. 7. 3. 4. 7. 9.\n"," 5. 6. 2. 5. 3. 9. 3. 9. 9. 0. 5. 2. 6. 9. 9. 4. 8. 7. 4. 3. 2. 4. 0. 3.\n"," 3. 5. 2. 4. 4. 2. 5. 7. 9. 8. 9. 9. 7. 4. 1. 3. 0. 8. 0. 8. 7. 4. 8. 4.\n"," 9. 0. 1. 1. 8. 2. 4. 9. 1. 9. 1. 8. 2. 1. 1. 7. 9. 7. 4. 2. 1. 1. 9. 3.\n"," 7. 5. 1. 8. 8. 6. 0. 6. 3. 4. 7. 9. 3. 4. 7. 9. 2. 4. 4. 3. 8. 8. 7. 5.]\n","[9. 8. 6. 2. 1. 0. 7. 3. 7. 7. 0. 6. 8. 1. 4. 2. 2. 5. 8. 4. 7. 8. 2. 2.\n"," 8. 2. 2. 7. 4. 9. 6. 0. 1. 1. 0. 4. 2. 2. 2. 4. 4. 1. 4. 5. 6. 9. 0. 0.\n"," 7. 0. 8. 5. 4. 4. 2. 8. 8. 6. 5. 0. 2. 7. 9. 9. 3. 6. 2. 4. 8. 0. 6. 6.\n"," 9. 2. 9. 6. 3. 1. 4. 5. 5. 5. 7. 2. 8. 2. 9. 1. 8. 3. 3. 8. 8. 0. 9. 1.\n"," 1. 8. 2. 0. 4. 9. 0. 1. 4. 8. 1. 9. 3. 1. 2. 9. 9. 8. 3. 9. 9. 2. 4. 3.\n"," 8. 1. 1. 4. 7. 2. 9. 0. 0. 4. 1. 4. 1. 9. 2. 9. 6. 1. 1. 4. 3. 1. 1. 4.\n"," 7. 2. 2. 0. 2. 2. 6. 3. 3. 0. 7. 4. 7. 3. 7. 6. 8. 3. 5. 9. 3. 6. 5. 3.\n"," 6. 7. 9. 6. 1. 2. 8. 1. 6. 2. 6. 4. 0. 8. 5. 1. 3. 8. 4. 2. 0. 6. 1. 3.\n"," 1. 0. 4. 3. 0. 3. 2. 8. 8. 9. 2. 5. 2. 9. 6. 8. 3. 3. 7. 2. 3. 1. 6. 5.\n"," 7. 2. 2. 9. 1. 7. 6. 8. 3. 2. 4. 7. 9. 8. 6. 6. 6. 4. 0. 3. 1. 4. 1. 9.\n"," 2. 2. 9. 1. 5. 2. 9. 7. 4. 4. 1. 0. 9. 8. 2. 1. 4. 7. 6. 4. 4. 4. 5. 8.\n"," 3. 2. 9. 3. 0. 4. 6. 1. 1. 6. 0. 2. 9. 8. 6. 0. 4. 9. 0. 2. 9. 1. 7. 3.\n"," 3. 5. 3. 1. 6. 6. 0. 4. 0. 5. 9. 3. 5. 0. 8. 1. 2. 1. 6. 3. 4. 3. 7. 8.\n"," 1. 8. 8. 7. 1. 1. 8. 0. 3. 1. 1. 4. 6. 5. 3. 7. 3. 7. 6. 1. 8. 0. 0. 0.\n"," 1. 9. 4. 4. 3. 5. 3. 3. 5. 9. 3. 4. 4. 5. 5. 7. 2. 1. 5. 7. 9. 2. 6. 0.\n"," 1. 6. 5. 7. 2. 4. 2. 5. 1. 2. 3. 7. 6. 1. 9. 8. 9. 7. 2. 2. 8. 0. 5. 4.\n"," 1. 2. 7. 4. 4. 0. 4. 4. 3. 1. 7. 8. 3. 1. 3. 3. 6. 0. 7. 9. 3. 3. 3. 1.\n"," 4. 9. 9. 6. 7. 8. 4. 6. 7. 2. 1. 3. 7. 4. 3. 5. 6. 0. 3. 9. 9. 3. 0. 8.\n"," 8. 0. 7. 6. 0. 9. 7. 0. 9. 7. 6. 9. 6. 8. 4. 8. 7. 7. 2. 2. 9. 7. 4. 2.\n"," 3. 9. 8. 7. 9. 1. 8. 1. 1. 3. 1. 7. 4. 1. 3. 1. 8. 5. 9. 3. 4. 7. 2. 2.\n"," 8. 1. 3. 4. 3. 8. 3. 3. 3. 9. 4. 2. 3. 4. 4. 1. 2. 3. 6. 2. 9. 1. 6. 4.\n"," 1. 6. 8. 9. 9. 8. 0. 5. 0. 0. 1. 7. 7. 4. 5. 4. 6. 1. 4. 8. 7. 4. 1. 6.\n"," 9. 4. 3. 8. 0. 8. 4. 9. 6. 1. 3. 3. 4. 4. 2. 2. 4. 2. 6. 5. 1. 2. 8. 8.\n"," 7. 6. 3. 5. 3. 2. 3. 0. 7. 8. 8. 6. 9. 5. 1. 8. 5. 6. 0. 9. 6. 9. 5. 8.\n"," 4. 2. 2. 2. 1. 5. 9. 8. 9. 0. 8. 6. 6. 1. 1. 2. 3. 1. 7. 8. 2. 9. 7. 6.]\n","[2. 7. 6. 7. 3. 3. 2. 8. 9. 6. 1. 5. 2. 8. 6. 3. 0. 6. 7. 5. 3. 3. 1. 3.\n"," 8. 6. 2. 7. 8. 4. 7. 1. 2. 9. 6. 7. 3. 5. 9. 8. 1. 2. 5. 9. 8. 8. 9. 1.\n"," 9. 9. 4. 2. 1. 4. 9. 6. 6. 8. 8. 4. 7. 4. 5. 8. 2. 5. 2. 0. 8. 7. 9. 0.\n"," 8. 0. 0. 2. 9. 7. 2. 8. 0. 9. 4. 7. 8. 2. 7. 8. 5. 5. 4. 5. 0. 1. 5. 5.\n"," 4. 9. 2. 1. 0. 4. 4. 1. 9. 3. 7. 1. 3. 0. 8. 0. 3. 6. 9. 1. 3. 3. 3. 5.\n"," 0. 0. 7. 7. 6. 8. 1. 3. 8. 4. 6. 9. 9. 2. 3. 0. 6. 6. 4. 3. 9. 9. 5. 8.\n"," 2. 2. 0. 3. 4. 5. 2. 0. 5. 5. 2. 5. 6. 3. 5. 9. 7. 6. 4. 7. 0. 0. 1. 7.\n"," 1. 5. 4. 8. 2. 4. 1. 2. 8. 2. 8. 0. 8. 9. 2. 1. 1. 6. 1. 5. 4. 1. 6. 9.\n"," 2. 8. 8. 6. 8. 7. 8. 1. 2. 0. 6. 3. 9. 8. 9. 1. 9. 1. 5. 4. 1. 0. 3. 4.\n"," 4. 5. 1. 1. 8. 9. 0. 3. 9. 5. 8. 8. 7. 9. 5. 6. 0. 9. 3. 7. 1. 4. 5. 8.\n"," 6. 8. 2. 9. 5. 3. 2. 1. 2. 9. 7. 0. 4. 3. 7. 4. 9. 4. 5. 2. 2. 9. 4. 8.\n"," 1. 7. 0. 5. 1. 7. 8. 7. 9. 4. 6. 2. 6. 3. 9. 3. 9. 9. 9. 4. 4. 6. 9. 1.\n"," 3. 9. 5. 9. 3. 0. 7. 9. 2. 3. 7. 7. 6. 1. 2. 8. 1. 9. 5. 5. 9. 1. 0. 6.\n"," 3. 1. 2. 0. 4. 5. 9. 3. 0. 3. 4. 4. 2. 9. 6. 8. 3. 0. 3. 4. 2. 2. 5. 5.\n"," 9. 3. 3. 4. 7. 1. 1. 1. 4. 9. 2. 3. 5. 0. 4. 2. 9. 9. 5. 9. 7. 5. 2. 0.\n"," 9. 5. 4. 0. 4. 4. 3. 5. 4. 5. 5. 9. 1. 1. 2. 9. 1. 2. 8. 8. 0. 8. 4. 2.\n"," 5. 3. 4. 0. 1. 7. 2. 0. 3. 2. 3. 0. 6. 1. 5. 3. 6. 1. 0. 5. 5. 9. 2. 1.\n"," 3. 1. 5. 3. 1. 3. 3. 0. 3. 4. 2. 2. 8. 3. 8. 0. 6. 9. 5. 9. 0. 8. 0. 1.\n"," 4. 1. 1. 9. 8. 9. 3. 1. 8. 6. 3. 3. 1. 7. 6. 8. 3. 2. 1. 7. 5. 7. 0. 8.\n"," 0. 6. 3. 7. 6. 9. 3. 6. 0. 5. 4. 4. 0. 2. 2. 1. 6. 8. 3. 2. 8. 1. 4. 8.\n"," 3. 7. 2. 3. 2. 2. 4. 0. 3. 5. 4. 2. 4. 9. 5. 9. 6. 8. 8. 1. 6. 3. 8. 0.\n"," 4. 6. 5. 4. 6. 3. 0. 0. 3. 0. 9. 7. 0. 9. 7. 3. 9. 8. 9. 1. 2. 2. 7. 1.\n"," 2. 8. 1. 0. 3. 8. 8. 1. 3. 7. 0. 2. 1. 9. 2. 1. 5. 0. 9. 7. 9. 5. 4. 5.\n"," 9. 1. 3. 7. 3. 7. 6. 5. 7. 9. 9. 6. 1. 1. 7. 4. 4. 5. 7. 0. 2. 1. 1. 7.\n"," 4. 4. 3. 7. 6. 7. 9. 3. 8. 4. 1. 7. 9. 6. 5. 0. 0. 2. 7. 2. 7. 5. 2. 9.]\n","[3. 8. 7. 7. 2. 5. 7. 2. 8. 9. 4. 4. 6. 8. 8. 0. 7. 1. 1. 1. 6. 0. 4. 6.\n"," 3. 2. 2. 4. 5. 5. 0. 2. 7. 4. 6. 7. 2. 5. 3. 2. 6. 1. 9. 5. 1. 2. 9. 8.\n"," 1. 7. 6. 7. 7. 0. 1. 0. 4. 2. 9. 6. 4. 4. 8. 5. 9. 1. 2. 4. 4. 4. 0. 7.\n"," 8. 9. 9. 0. 9. 1. 9. 1. 6. 0. 6. 1. 3. 3. 1. 4. 7. 7. 1. 7. 5. 5. 0. 8.\n"," 5. 2. 3. 4. 8. 7. 0. 2. 3. 5. 8. 3. 3. 9. 6. 5. 4. 6. 8. 5. 8. 1. 6. 0.\n"," 6. 7. 6. 0. 0. 2. 4. 0. 1. 4. 3. 6. 3. 6. 8. 1. 4. 3. 5. 2. 8. 1. 5. 1.\n"," 7. 0. 0. 5. 0. 2. 2. 9. 1. 0. 7. 3. 1. 5. 6. 6. 9. 6. 8. 2. 9. 8. 2. 6.\n"," 4. 3. 3. 1. 0. 4. 6. 4. 4. 0. 8. 3. 6. 5. 9. 2. 9. 3. 1. 9. 5. 2. 6. 7.\n"," 3. 7. 7. 7. 9. 8. 0. 6. 9. 5. 2. 0. 3. 2. 3. 8. 9. 5. 4. 4. 7. 4. 0. 4.\n"," 1. 4. 6. 6. 1. 3. 5. 0. 3. 0. 4. 2. 3. 4. 7. 9. 6. 1. 7. 2. 2. 2. 0. 9.\n"," 5. 7. 3. 5. 0. 2. 8. 9. 7. 0. 8. 1. 7. 3. 5. 9. 8. 5. 4. 6. 5. 6. 2. 7.\n"," 6. 8. 1. 1. 2. 3. 9. 5. 8. 7. 2. 5. 3. 1. 9. 7. 4. 7. 6. 2. 6. 5. 1. 0.\n"," 6. 8. 9. 3. 3. 1. 0. 6. 0. 1. 6. 2. 2. 7. 1. 4. 0. 2. 6. 1. 5. 1. 9. 9.\n"," 3. 3. 7. 7. 5. 1. 6. 2. 3. 5. 1. 5. 5. 3. 9. 9. 7. 3. 3. 9. 5. 1. 7. 0.\n"," 2. 4. 1. 3. 8. 3. 0. 7. 4. 8. 6. 6. 6. 2. 8. 7. 7. 1. 0. 9. 8. 2. 4. 5.\n"," 9. 1. 6. 3. 1. 0. 9. 8. 3. 7. 2. 3. 4. 6. 6. 3. 7. 6. 8. 8. 2. 3. 4. 0.\n"," 3. 8. 2. 7. 1. 0. 9. 2. 3. 9. 0. 6. 4. 8. 0. 6. 8. 7. 0. 5. 2. 8. 9. 8.\n"," 8. 9. 0. 0. 6. 1. 6. 9. 3. 8. 4. 7. 8. 9. 7. 3. 5. 2. 6. 9. 6. 3. 7. 3.\n"," 7. 0. 7. 0. 4. 7. 7. 2. 4. 8. 8. 1. 7. 3. 2. 3. 8. 0. 4. 0. 2. 9. 1. 5.\n"," 7. 8. 8. 1. 3. 1. 7. 0. 2. 5. 8. 3. 9. 1. 3. 8. 8. 7. 0. 4. 6. 8. 3. 2.\n"," 0. 4. 5. 5. 6. 4. 8. 5. 1. 0. 8. 9. 1. 9. 7. 3. 6. 6. 9. 0. 9. 3. 8. 9.\n"," 3. 3. 2. 0. 3. 6. 4. 1. 7. 2. 8. 8. 1. 8. 1. 4. 7. 0. 2. 9. 0. 0. 6. 7.\n"," 4. 6. 9. 3. 7. 8. 4. 8. 4. 0. 4. 7. 2. 6. 4. 5. 3. 1. 0. 5. 6. 2. 6. 8.\n"," 7. 6. 4. 3. 4. 6. 1. 1. 1. 9. 4. 4. 1. 5. 6. 6. 9. 7. 1. 1. 8. 2. 7. 1.\n"," 4. 6. 1. 3. 1. 9. 5. 9. 3. 7. 8. 4. 8. 9. 4. 9. 5. 3. 8. 0. 1. 6. 5. 3.]\n","[4. 4. 2. 5. 7. 0. 6. 7. 0. 1. 3. 1. 7. 0. 4. 2. 3. 8. 4. 9. 6. 7. 4. 3.\n"," 7. 0. 7. 6. 9. 0. 4. 4. 6. 1. 0. 1. 8. 2. 3. 0. 5. 7. 6. 3. 1. 1. 3. 4.\n"," 7. 7. 9. 9. 8. 8. 4. 1. 2. 2. 7. 1. 8. 1. 9. 2. 0. 5. 1. 2. 4. 8. 3. 0.\n"," 9. 8. 9. 3. 0. 5. 0. 9. 8. 7. 6. 3. 3. 2. 1. 7. 6. 2. 3. 3. 1. 9. 0. 3.\n"," 1. 1. 9. 8. 4. 5. 8. 3. 5. 3. 8. 7. 4. 8. 8. 2. 8. 9. 6. 8. 6. 7. 0. 7.\n"," 4. 7. 0. 8. 1. 7. 8. 7. 9. 4. 0. 6. 9. 7. 4. 6. 5. 5. 8. 1. 4. 0. 3. 2.\n"," 3. 3. 9. 1. 8. 4. 7. 7. 1. 9. 2. 1. 6. 1. 6. 0. 1. 1. 6. 2. 2. 5. 4. 1.\n"," 3. 6. 7. 1. 6. 2. 7. 3. 5. 2. 3. 3. 6. 2. 5. 7. 3. 1. 2. 1. 1. 2. 8. 2.\n"," 7. 7. 1. 1. 9. 3. 2. 0. 2. 8. 9. 3. 1. 4. 3. 6. 4. 8. 4. 1. 5. 5. 3. 8.\n"," 2. 5. 6. 6. 8. 7. 9. 6. 9. 0. 6. 3. 6. 4. 9. 7. 2. 9. 4. 8. 5. 0. 9. 5.\n"," 6. 6. 1. 3. 8. 6. 2. 7. 4. 8. 0. 6. 0. 3. 6. 8. 4. 1. 5. 9. 6. 2. 7. 7.\n"," 1. 6. 9. 0. 7. 0. 0. 2. 5. 2. 4. 8. 1. 3. 2. 7. 2. 8. 1. 6. 4. 6. 2. 1.\n"," 6. 7. 6. 3. 7. 7. 5. 4. 5. 5. 5. 1. 6. 1. 7. 0. 3. 9. 7. 3. 8. 3. 6. 0.\n"," 8. 5. 5. 1. 2. 3. 5. 5. 7. 5. 9. 6. 1. 8. 5. 3. 7. 8. 0. 2. 8. 3. 4. 8.\n"," 3. 3. 1. 8. 7. 2. 8. 9. 4. 7. 7. 3. 9. 4. 0. 3. 3. 5. 8. 7. 9. 5. 4. 5.\n"," 9. 3. 9. 7. 8. 3. 2. 0. 5. 2. 1. 1. 1. 1. 8. 8. 0. 8. 2. 4. 3. 6. 5. 3.\n"," 5. 1. 3. 4. 6. 9. 0. 8. 8. 0. 0. 1. 8. 8. 3. 9. 1. 4. 5. 0. 7. 1. 0. 2.\n"," 8. 1. 7. 2. 5. 1. 7. 3. 9. 3. 2. 0. 7. 2. 1. 4. 6. 6. 8. 5. 3. 0. 9. 5.\n"," 5. 8. 0. 2. 2. 6. 8. 0. 8. 9. 9. 9. 6. 1. 7. 1. 1. 9. 7. 9. 3. 4. 9. 6.\n"," 1. 8. 5. 7. 7. 1. 8. 0. 1. 7. 5. 1. 7. 9. 0. 5. 3. 5. 5. 8. 7. 4. 8. 8.\n"," 6. 6. 0. 0. 9. 7. 0. 4. 2. 2. 2. 5. 7. 3. 1. 8. 2. 5. 2. 7. 8. 9. 8. 3.\n"," 9. 5. 2. 5. 0. 9. 8. 1. 6. 7. 2. 0. 9. 4. 7. 0. 3. 3. 6. 3. 3. 8. 9. 8.\n"," 9. 3. 2. 2. 7. 2. 9. 7. 4. 3. 9. 6. 3. 5. 0. 9. 6. 5. 3. 1. 0. 2. 5. 6.\n"," 1. 9. 7. 0. 4. 1. 3. 3. 2. 1. 2. 3. 3. 9. 6. 6. 8. 7. 4. 7. 5. 0. 9. 5.\n"," 0. 7. 7. 2. 7. 1. 8. 8. 2. 0. 9. 6. 3. 3. 8. 5. 3. 6. 1. 1. 1. 2. 0. 3.]\n","[5. 2. 4. 5. 6. 8. 4. 6. 6. 5. 6. 4. 0. 6. 1. 2. 0. 7. 4. 0. 3. 1. 3. 6.\n"," 7. 8. 3. 2. 0. 3. 8. 5. 5. 2. 6. 4. 7. 6. 1. 5. 1. 8. 2. 2. 1. 4. 6. 0.\n"," 1. 5. 5. 2. 3. 3. 7. 2. 9. 5. 9. 8. 4. 5. 8. 1. 6. 0. 4. 9. 3. 1. 4. 5.\n"," 0. 3. 2. 5. 7. 3. 0. 2. 1. 4. 7. 2. 2. 4. 9. 7. 3. 9. 3. 3. 3. 3. 2. 0.\n"," 5. 6. 7. 4. 2. 2. 5. 1. 6. 2. 8. 9. 8. 6. 7. 7. 6. 1. 7. 3. 9. 9. 7. 0.\n"," 6. 1. 0. 0. 7. 9. 8. 5. 4. 3. 2. 2. 9. 2. 0. 1. 5. 7. 1. 1. 7. 3. 8. 0.\n"," 5. 0. 7. 8. 0. 1. 8. 6. 8. 7. 1. 7. 3. 3. 3. 6. 6. 9. 5. 9. 1. 6. 4. 7.\n"," 1. 1. 7. 3. 9. 9. 4. 4. 3. 5. 1. 8. 8. 1. 1. 9. 3. 2. 8. 2. 6. 4. 8. 3.\n"," 1. 9. 3. 4. 7. 0. 4. 5. 8. 6. 9. 8. 9. 1. 3. 4. 2. 3. 1. 0. 2. 2. 0. 2.\n"," 8. 4. 1. 9. 3. 3. 0. 5. 1. 2. 1. 2. 2. 8. 3. 4. 1. 7. 4. 3. 9. 4. 7. 0.\n"," 1. 5. 3. 9. 3. 7. 0. 9. 0. 4. 5. 9. 2. 1. 0. 0. 0. 5. 6. 0. 2. 0. 0. 5.\n"," 5. 6. 2. 0. 1. 3. 7. 6. 9. 1. 4. 2. 0. 0. 2. 5. 5. 7. 5. 1. 4. 1. 2. 4.\n"," 7. 4. 1. 2. 0. 9. 4. 3. 7. 6. 7. 1. 6. 2. 4. 1. 4. 7. 2. 4. 3. 6. 5. 4.\n"," 9. 5. 1. 3. 3. 3. 2. 9. 1. 0. 6. 8. 4. 5. 1. 8. 4. 4. 0. 6. 4. 3. 0. 5.\n"," 5. 2. 6. 8. 9. 4. 4. 9. 7. 6. 0. 5. 2. 5. 4. 6. 9. 3. 0. 4. 8. 4. 3. 0.\n"," 0. 7. 4. 1. 1. 8. 4. 0. 9. 9. 6. 3. 4. 8. 1. 0. 7. 8. 3. 3. 5. 0. 7. 7.\n"," 4. 7. 1. 0. 3. 9. 9. 1. 7. 3. 2. 4. 2. 9. 0. 9. 5. 1. 5. 7. 2. 6. 0. 0.\n"," 6. 4. 0. 3. 2. 7. 7. 0. 5. 3. 9. 9. 8. 4. 4. 4. 3. 0. 1. 9. 8. 6. 4. 8.\n"," 9. 5. 5. 0. 8. 9. 1. 7. 1. 3. 7. 2. 4. 5. 7. 6. 3. 9. 1. 9. 4. 5. 6. 5.\n"," 6. 8. 7. 8. 7. 4. 6. 1. 4. 6. 3. 8. 1. 5. 5. 0. 6. 3. 5. 2. 6. 8. 1. 0.\n"," 0. 6. 6. 7. 7. 2. 1. 0. 9. 1. 0. 4. 5. 3. 5. 6. 7. 1. 0. 7. 5. 7. 1. 2.\n"," 7. 0. 3. 1. 4. 2. 0. 6. 9. 8. 5. 1. 0. 0. 6. 2. 5. 9. 7. 2. 2. 1. 6. 6.\n"," 1. 9. 1. 5. 3. 5. 9. 8. 8. 2. 0. 2. 3. 9. 6. 6. 1. 2. 6. 0. 2. 7. 3. 6.\n"," 3. 0. 6. 9. 4. 0. 5. 9. 3. 0. 0. 4. 3. 8. 1. 8. 9. 9. 4. 5. 5. 1. 3. 6.\n"," 9. 1. 1. 9. 9. 5. 1. 5. 1. 1. 8. 4. 8. 1. 2. 1. 0. 6. 5. 2. 4. 0. 3. 5.]\n"]}]}]}